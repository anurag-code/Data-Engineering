{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install ipython-sql for running magical sql commands :\n",
    "## %sql for single line commands and %%sql for multi line sql commands.\n",
    "## We can also use the variables defined in python program along with %sql command by using $ as a prefix to the python variables.\n",
    "\n",
    "## uncomment the pip command below to run it.\n",
    "\n",
    "#!pip install ipython-sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Libraries\n",
    "\n",
    "%reload_ext sql     \n",
    "## I have used reload intead of load because I ran this program multiple times. Simple load command will throw error when used multiple times.\n",
    "\n",
    "\n",
    "import psycopg2     ## Psycopg2 is the most popular PostgreSQL database adapter for the Python programming language.\n",
    "                    ## Since redshift is also a modified postgres db, thus it is used to access redshift from outside the VPC(virtual private cloud (aws)).\n",
    "    \n",
    "    \n",
    "import pandas as pd  \n",
    "import boto3        ## Boto3 is the Amazon Web Services (AWS) Software Development Kit (SDK) for Python.\n",
    "                    ## It allows Python developers to write software that makes use of services like Amazon S3 ,IAM, and  Amazon EC2 etc.\n",
    "    \n",
    "    \n",
    "import json          \n",
    "import configparser  ## This is used to programmatically access the configuration file (dwh.cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Param</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DWH_CLUSTER_TYPE</td>\n",
       "      <td>multi-node</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DWH_NUM_NODES</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DWH_NODE_TYPE</td>\n",
       "      <td>dc2.large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DWH_CLUSTER_IDENTIFIER</td>\n",
       "      <td>dwh-cluster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DWH_DB</td>\n",
       "      <td>dwh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DWH_DB_USER</td>\n",
       "      <td>dwhuser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DWH_DB_PASSWORD</td>\n",
       "      <td>Passw0rd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DWH_PORT</td>\n",
       "      <td>5439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DWH_IAM_ROLE_NAME</td>\n",
       "      <td>dwhRole</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Param        Value\n",
       "0        DWH_CLUSTER_TYPE   multi-node\n",
       "1           DWH_NUM_NODES            4\n",
       "2           DWH_NODE_TYPE    dc2.large\n",
       "3  DWH_CLUSTER_IDENTIFIER  dwh-cluster\n",
       "4                  DWH_DB          dwh\n",
       "5             DWH_DB_USER      dwhuser\n",
       "6         DWH_DB_PASSWORD     Passw0rd\n",
       "7                DWH_PORT         5439\n",
       "8       DWH_IAM_ROLE_NAME      dwhRole"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Access the config file\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Save the credentials in the form of variables\n",
    "\n",
    "KEY                    = config.get('AWS','KEY')\n",
    "SECRET                 = config.get('AWS','SECRET')\n",
    "\n",
    "DWH_CLUSTER_TYPE       = config.get(\"DWH\",\"DWH_CLUSTER_TYPE\")\n",
    "DWH_NUM_NODES          = config.get(\"DWH\",\"DWH_NUM_NODES\")\n",
    "DWH_NODE_TYPE          = config.get(\"DWH\",\"DWH_NODE_TYPE\")\n",
    "\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\")\n",
    "DWH_DB                 = config.get(\"DWH\",\"DWH_DB\")\n",
    "DWH_DB_USER            = config.get(\"DWH\",\"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD        = config.get(\"DWH\",\"DWH_DB_PASSWORD\")\n",
    "DWH_PORT               = config.get(\"DWH\",\"DWH_PORT\")\n",
    "\n",
    "DWH_IAM_ROLE_NAME      = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\")\n",
    "\n",
    "\n",
    "\n",
    "## Create a dataframe for the configuration parameters, just to have a quick look.\n",
    "pd.DataFrame({\"Param\":\n",
    "                  [\"DWH_CLUSTER_TYPE\", \"DWH_NUM_NODES\", \"DWH_NODE_TYPE\", \"DWH_CLUSTER_IDENTIFIER\", \"DWH_DB\", \"DWH_DB_USER\", \"DWH_DB_PASSWORD\", \"DWH_PORT\", \"DWH_IAM_ROLE_NAME\"],\n",
    "              \"Value\":\n",
    "                  [DWH_CLUSTER_TYPE, DWH_NUM_NODES, DWH_NODE_TYPE, DWH_CLUSTER_IDENTIFIER, DWH_DB, DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT, DWH_IAM_ROLE_NAME]\n",
    "             })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Access clients and resources in aws vpc from jupyter\n",
    "\n",
    "## Create boto3 sdk objects for connecting to aws S3 , IAM, and redshift through boto3, on behalf of the user (created and named as dwhadmin in aws).\n",
    "## It (user: dwhadmin) has associated key and secret.\n",
    "\n",
    "## boto3 actually offers two different styles of API ‘’Resource API’’ (high-level, recommended) and ‘’Client API ‘’(low-level). \n",
    "#### You can refer to the links for details. More about boto3: https://boto3.readthedocs.io/en/latest/reference/services/s3.html\n",
    "#### https://medium.com/@rogerxujiang/use-s3-storage-on-aws-c4e5ce4fa46e\n",
    "\n",
    "## --------------------------------------------------------------------\n",
    "\n",
    "## Create an object s3 through which we can access the s3 buckets in aws.\n",
    "\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                       region_name=\"ap-southeast-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                   )\n",
    "\n",
    "\n",
    "## Create an object iam through which we can access the iam roles in aws. It has not been used , but created just to show the procedure.\n",
    "\n",
    "\n",
    "iam = boto3.client('iam',aws_access_key_id=KEY,\n",
    "                     aws_secret_access_key=SECRET,\n",
    "                     region_name='ap-southeast-2'\n",
    "                  )\n",
    "\n",
    "\n",
    "## Create an object redshift through which we can access redshift in aws. It has not been used , but created just to show the procedure.\n",
    "\n",
    "\n",
    "redshift = boto3.client('redshift',\n",
    "                       region_name=\"ap-southeast-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "boto3.resources.factory.s3.ServiceResource"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(s3)  ## boto3 S3 object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "botocore.client.IAM"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "botocore.client.Redshift"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(redshift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['log-data/',\n",
       " 'log-data/2018/11/2018-11-01-events.json',\n",
       " 'log-data/2018/11/2018-11-02-events.json',\n",
       " 'log-data/2018/11/2018-11-03-events.json',\n",
       " 'log-data/2018/11/2018-11-04-events.json',\n",
       " 'log-data/2018/11/2018-11-05-events.json',\n",
       " 'log-data/2018/11/2018-11-06-events.json',\n",
       " 'log-data/2018/11/2018-11-07-events.json',\n",
       " 'log-data/2018/11/2018-11-08-events.json',\n",
       " 'log-data/2018/11/2018-11-09-events.json']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Connect to a S3 bucket in aws vpc through BOTO3 object S3 by creating bucket object.\n",
    "\n",
    "\n",
    "bucket=s3.Bucket('udacity-dend')\n",
    "\n",
    "\n",
    "## Access the log data files in the s3 bucket 'udacity-dend' , and filter the files based on the prefix.\n",
    "\n",
    "\n",
    "log_data_files = [filename.key for filename in bucket.objects.filter(Prefix='log-data')]\n",
    "log_data_files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['song-data/A/A/A/TRAAAAK128F9318786.json',\n",
       " 'song-data/A/A/A/TRAAAAV128F421A322.json',\n",
       " 'song-data/A/A/A/TRAAABD128F429CF47.json',\n",
       " 'song-data/A/A/A/TRAAACN128F9355673.json',\n",
       " 'song-data/A/A/A/TRAAAEA128F935A30D.json',\n",
       " 'song-data/A/A/A/TRAAAED128E0783FAB.json',\n",
       " 'song-data/A/A/A/TRAAAEM128F93347B9.json',\n",
       " 'song-data/A/A/A/TRAAAEW128F42930C0.json',\n",
       " 'song-data/A/A/A/TRAAAFD128F92F423A.json',\n",
       " 'song-data/A/A/A/TRAAAGR128F425B14B.json']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Access the song data files in the s3 bucket 'udacity-dend', and filter the files based on the prefix.\n",
    "\n",
    "\n",
    "song_data_files = [filename.key for filename in bucket.objects.filter(Prefix='song-data/A')]\n",
    "song_data_files[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing connection with the Redshift DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Variables for redshift DB connectivity from config file.\n",
    "LOG_DATA          = config.get(\"S3\",\"LOG_DATA\")\n",
    "LOG_PATH          = config.get(\"S3\",\"LOG_JSONPATH\")\n",
    "SONG_DATA         = config.get(\"S3\",\"SONG_DATA\")\n",
    "IAM_ROLE          = config.get(\"IAM_ROLE\",\"ARN\")\n",
    "HOST              = config.get(\"CLUSTER\",\"HOST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connection between python and redshift can be done in multiple ways:\n",
    "\n",
    "#### 1. Using psycopg2:  https://www.blendo.co/blog/access-your-data-in-amazon-redshift-and-postgresql-with-python-and-r/\n",
    "\n",
    "#### 2. Using SQLAlchemy : https://pypi.org/project/sqlalchemy-redshift/  && https://www.compose.com/articles/using-postgresql-through-sqlalchemy/\n",
    "\n",
    "#### 3. Using simple SQL using URL as given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Format for calling theURL: postgresql://username:password@host:port/databasename\n",
    "## AWS Documentation for Copying Data to Redshift:  https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html#r_COPY-syntax-overview-data-source\n",
    "## https://docs.aws.amazon.com/redshift/latest/dg/t_Loading_tables_with_the_COPY_command.html\n",
    "\n",
    "\n",
    "\n",
    "conn_string = \"postgresql://{}:{}@{}:{}/{}\".format(DWH_DB_USER, DWH_DB_PASSWORD, HOST, DWH_PORT, DWH_DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql://dwhuser:Passw0rd@dwh-cluster.culrjdfmjjzn.ap-southeast-2.redshift.amazonaws.com:5439/dwh\n"
     ]
    }
   ],
   "source": [
    "print(conn_string) # Check the format in the print output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connected: dwhuser@dwh'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The output should be 'Connected: user_name@db_name'. \n",
    "## Important: It only gets connected when we run the object conn_string or call the get request using the above link.\n",
    "\n",
    "%sql $conn_string  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Staging Tables and Star Schema Tables to create a data warehouse\n",
    "\n",
    "#### We can efficiently update and insert new data by loading our data into a staging table first in the redshift\n",
    "#### The staging table is a temporary table that holds all of the data that will be used to make changes to the target table, including both updates and inserts.\n",
    "#### https://docs.aws.amazon.com/redshift/latest/dg/merge-create-staging-table.html\n",
    "#### https://docs.aws.amazon.com/redshift/latest/dg/t_updating-inserting-using-staging-tables-.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwh-cluster.culrjdfmjjzn.ap-southeast-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      " * postgresql://dwhuser:***@dwh-cluster.culrjdfmjjzn.ap-southeast-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DROP TABLES if they already exist.\n",
    "\n",
    "staging_events_table_drop = \"DROP TABLE IF EXISTS staging_events\"   \n",
    "staging_songs_table_drop  = \"DROP TABLE IF EXISTS staging_songs\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Our query is stored in the variable and by using %sql we are running the query stored in the variable using $ sign with the variable.\n",
    "\n",
    "\n",
    "%sql $staging_events_table_drop\n",
    "%sql $staging_songs_table_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREATE Staging TABLES: \n",
    "\n",
    "staging_events_table_create= (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS staging_events\n",
    "(\n",
    "artist          VARCHAR,\n",
    "auth            VARCHAR, \n",
    "firstName       VARCHAR,\n",
    "gender          VARCHAR,   \n",
    "itemInSession   INTEGER,\n",
    "lastName        VARCHAR,\n",
    "length          FLOAT,\n",
    "level           VARCHAR, \n",
    "location        VARCHAR,\n",
    "method          VARCHAR,\n",
    "page            VARCHAR,\n",
    "registration    BIGINT,\n",
    "sessionId       INTEGER,\n",
    "song            VARCHAR,\n",
    "status          INTEGER,\n",
    "ts              TIMESTAMP,\n",
    "userAgent       VARCHAR,\n",
    "userId          INTEGER\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "staging_songs_table_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS staging_songs\n",
    "(\n",
    "song_id            VARCHAR,\n",
    "num_songs          INTEGER,\n",
    "title              VARCHAR,\n",
    "artist_name        VARCHAR,\n",
    "artist_latitude    FLOAT,\n",
    "year               INTEGER,\n",
    "duration           FLOAT,\n",
    "artist_id          VARCHAR,\n",
    "artist_longitude   FLOAT,\n",
    "artist_location    VARCHAR\n",
    ");\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwh-cluster.culrjdfmjjzn.ap-southeast-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      " * postgresql://dwhuser:***@dwh-cluster.culrjdfmjjzn.ap-southeast-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql $staging_events_table_create\n",
    "%sql $staging_songs_table_create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Staging Tables into the redshift Database by using COPY command.\n",
    "### AWS Documentation for Copying Data to Redshift: \n",
    "#### https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html#r_COPY-syntax-overview-data-source\n",
    "\n",
    "### Article for 'when to use 'auto'and when to use 'Jsonpath' :\n",
    "#### https://sonra.io/2019/04/24/working-with-json-in-redshift-options-limitations-and-alternatives/\n",
    "\n",
    "\n",
    "### Other useful links:\n",
    "#### https://docs.aws.amazon.com/redshift/latest/dg/t_loading-tables-from-s3.html\n",
    "\n",
    "#### https://docs.aws.amazon.com/redshift/latest/dg/tutorial-loading-data.html\n",
    "\n",
    "### The staging table is be used to create the data-warehouse -\n",
    "### -(here we are creating only one datamart in data warehouse) having a star schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A log Json path file has been created and saved in S3 because there are two ways of copying the json files to redshift:\n",
    "## 1. auto:       This technique is used when all the names of keys of json file's key val pairs are same as that of the column names in schema.\n",
    "##                In this column mapping is directly done based on matching keys. values are stored corresponding to matching key and column names.\n",
    "## 2. Json_path: This technique is used when the column names are different to the name of keys in Json file. \n",
    "##               So in these cases, we create a seperate Json mapping file to map the keys to columns. \n",
    "## refer this article for difference between auto anf jsonpath : \n",
    "##  https://sonra.io/2019/04/24/working-with-json-in-redshift-options-limitations-and-alternatives/\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Here we are downloading the json path file from S3 to see its content.\n",
    "\n",
    "s3.Object(bucket_name='udacity-dend', key='log_json_path.json').download_file('log_json_path_download.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGING TABLES\n",
    "\n",
    "## Remember the S3 buckets we are calling here is in US.\n",
    "## For staging_event we have used json path technique while for staging_songs we have used auto.\n",
    "## In case of staging_events the column names didnt match the keys of json files thus we have used the jsonpath technique.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "staging_events_copy = (\"\"\"\n",
    "    COPY staging_events FROM {}\n",
    "    CREDENTIALS 'aws_iam_role={}'\n",
    "    COMPUPDATE OFF region 'us-west-2'\n",
    "    TIMEFORMAT as 'epochmillisecs'\n",
    "    TRUNCATECOLUMNS BLANKSASNULL EMPTYASNULL\n",
    "    FORMAT AS JSON {}\n",
    "\"\"\").format(LOG_DATA, IAM_ROLE,LOG_PATH)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "staging_songs_copy = (\"\"\"\n",
    "    COPY staging_songs FROM {}\n",
    "    CREDENTIALS 'aws_iam_role={}'\n",
    "    COMPUPDATE OFF region 'us-west-2'\n",
    "    FORMAT AS JSON 'auto' \n",
    "    TRUNCATECOLUMNS BLANKSASNULL EMPTYASNULL;\n",
    "\"\"\").format(SONG_DATA, IAM_ROLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwh-cluster.culrjdfmjjzn.ap-southeast-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      " * postgresql://dwhuser:***@dwh-cluster.culrjdfmjjzn.ap-southeast-2.redshift.amazonaws.com:5439/dwh\n"
     ]
    }
   ],
   "source": [
    "## Our query is stored in the variable and by using %sql we are running the query stored in the variable using $ sign with the variable.\n",
    "\n",
    "\n",
    "%sql $staging_events_copy     \n",
    "\n",
    "%sql $staging_songs_copy      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "select * from staging_songs limit 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "select * from staging_events limit 20"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
