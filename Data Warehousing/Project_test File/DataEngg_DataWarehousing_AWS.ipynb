{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipython-sql in c:\\users\\w38364\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (0.3.9)\n",
      "Requirement already satisfied: prettytable in c:\\users\\w38364\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from ipython-sql) (0.7.2)\n",
      "Requirement already satisfied: sqlalchemy>=0.6.7 in c:\\users\\w38364\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from ipython-sql) (1.2.11)\n",
      "Requirement already satisfied: ipython-genutils>=0.1.0 in c:\\users\\w38364\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from ipython-sql) (0.2.0)\n",
      "Requirement already satisfied: ipython>=1.0 in c:\\users\\w38364\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from ipython-sql) (6.5.0)\n",
      "Requirement already satisfied: sqlparse in c:\\users\\w38364\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from ipython-sql) (0.3.0)\n",
      "Requirement already satisfied: six in c:\\users\\w38364\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from ipython-sql) (1.11.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.15 in c:\\users\\w38364\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from ipython>=1.0->ipython-sql) (1.0.15)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\users\\w38364\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from ipython>=1.0->ipython-sql) (0.3.9)\n",
      "Requirement already satisfied: backcall in c:\\users\\w38364\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from ipython>=1.0->ipython-sql) (0.1.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in c:\\users\\w38364\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from ipython>=1.0->ipython-sql) (4.3.2)\n",
      "Requirement already satisfied: pygments in c:\\users\\w38364\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from ipython>=1.0->ipython-sql) (2.2.0)\n",
      "Requirement already satisfied: simplegeneric>0.8 in c:\\users\\w38364\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from ipython>=1.0->ipython-sql) (0.8.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\w38364\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from ipython>=1.0->ipython-sql) (40.2.0)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\w38364\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from ipython>=1.0->ipython-sql) (0.7.4)\n",
      "Requirement already satisfied: jedi>=0.10 in c:\\users\\w38364\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from ipython>=1.0->ipython-sql) (0.12.1)\n",
      "Requirement already satisfied: decorator in c:\\users\\w38364\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from ipython>=1.0->ipython-sql) (4.3.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\w38364\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from prompt-toolkit<2.0.0,>=1.0.15->ipython>=1.0->ipython-sql) (0.1.7)\n",
      "Requirement already satisfied: parso>=0.3.0 in c:\\users\\w38364\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from jedi>=0.10->ipython>=1.0->ipython-sql) (0.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.3; however, version 19.3.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "## Install ipython-sql for running magical sql commands :\n",
    "## %sql for single line commands and %%sql for multi line sql commands.\n",
    "## We can also use the variables defined in python program along with %sql command by using $ as a prefix to the python variables.\n",
    "\n",
    "## uncomment the pip command below to run it.\n",
    "\n",
    "#!pip install ipython-sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Libraries\n",
    "\n",
    "%reload_ext sql     \n",
    "## I have used reload intead of load because I ran this program multiple times. Simple load command will throw error when used multiple times.\n",
    "import psycopg2     ## Psycopg2 is the most popular PostgreSQL database adapter for the Python programming language.\n",
    "                    ## Since redshift is also a modified postgres db, thus it is used to access redshift from outside the VPC(virtual private cloud (aws)).\n",
    "import pandas as pd  \n",
    "import boto3        ## Boto3 is the Amazon Web Services (AWS) Software Development Kit (SDK) for Python.\n",
    "                    ## It allows Python developers to write software that makes use of services like Amazon S3 ,IAM, and  Amazon EC2 etc.\n",
    "import json          \n",
    "import configparser  ## This is used to programmatically access the configuration file (dwh.cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Param</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DWH_CLUSTER_TYPE</td>\n",
       "      <td>multi-node</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DWH_NUM_NODES</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DWH_NODE_TYPE</td>\n",
       "      <td>dc2.large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DWH_CLUSTER_IDENTIFIER</td>\n",
       "      <td>dwh-cluster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DWH_DB</td>\n",
       "      <td>dwh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DWH_DB_USER</td>\n",
       "      <td>dwhuser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DWH_DB_PASSWORD</td>\n",
       "      <td>Passw0rd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DWH_PORT</td>\n",
       "      <td>5439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DWH_IAM_ROLE_NAME</td>\n",
       "      <td>dwhRole</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Param        Value\n",
       "0        DWH_CLUSTER_TYPE   multi-node\n",
       "1           DWH_NUM_NODES            4\n",
       "2           DWH_NODE_TYPE    dc2.large\n",
       "3  DWH_CLUSTER_IDENTIFIER  dwh-cluster\n",
       "4                  DWH_DB          dwh\n",
       "5             DWH_DB_USER      dwhuser\n",
       "6         DWH_DB_PASSWORD     Passw0rd\n",
       "7                DWH_PORT         5439\n",
       "8       DWH_IAM_ROLE_NAME      dwhRole"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Access the config file\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "\n",
    "## Save the credentials in the form of variables\n",
    "\n",
    "KEY                    = config.get('AWS','KEY')\n",
    "SECRET                 = config.get('AWS','SECRET')\n",
    "\n",
    "DWH_CLUSTER_TYPE       = config.get(\"DWH\",\"DWH_CLUSTER_TYPE\")\n",
    "DWH_NUM_NODES          = config.get(\"DWH\",\"DWH_NUM_NODES\")\n",
    "DWH_NODE_TYPE          = config.get(\"DWH\",\"DWH_NODE_TYPE\")\n",
    "\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\")\n",
    "DWH_DB                 = config.get(\"DWH\",\"DWH_DB\")\n",
    "DWH_DB_USER            = config.get(\"DWH\",\"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD        = config.get(\"DWH\",\"DWH_DB_PASSWORD\")\n",
    "DWH_PORT               = config.get(\"DWH\",\"DWH_PORT\")\n",
    "\n",
    "DWH_IAM_ROLE_NAME      = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\")\n",
    "\n",
    "(DWH_DB_USER, DWH_DB_PASSWORD, DWH_DB)\n",
    "\n",
    "pd.DataFrame({\"Param\":\n",
    "                  [\"DWH_CLUSTER_TYPE\", \"DWH_NUM_NODES\", \"DWH_NODE_TYPE\", \"DWH_CLUSTER_IDENTIFIER\", \"DWH_DB\", \"DWH_DB_USER\", \"DWH_DB_PASSWORD\", \"DWH_PORT\", \"DWH_IAM_ROLE_NAME\"],\n",
    "              \"Value\":\n",
    "                  [DWH_CLUSTER_TYPE, DWH_NUM_NODES, DWH_NODE_TYPE, DWH_CLUSTER_IDENTIFIER, DWH_DB, DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT, DWH_IAM_ROLE_NAME]\n",
    "             })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Access clients and resources in aws vpc from jupyter\n",
    "\n",
    "## Create boto3 objects for connecting to aws S3 , IAM, and redshift through boto3, on behalf of the user (created and named as dwhadmin in aws).\n",
    "## It (user: dwhadmin) has associated key and secret.\n",
    "\n",
    "## An easy way to understand the difference between the user and the client is as follows:\n",
    "## Resource: Something which is going to be used by the User. S3 will be used by User, and a pseudo user redshift (redshift will read the data from s3)\n",
    "## Client: Someone who use the resources such as User and pseudo users with assigned roles.\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                       region_name=\"ap-southeast-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                   )\n",
    "\n",
    "iam = boto3.client('iam',aws_access_key_id=KEY,\n",
    "                     aws_secret_access_key=SECRET,\n",
    "                     region_name='ap-southeast-2'\n",
    "                  )\n",
    "\n",
    "redshift = boto3.client('redshift',\n",
    "                       region_name=\"ap-southeast-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "boto3.resources.factory.s3.ServiceResource"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(s3)  ## boto3 S3 object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "botocore.client.IAM"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(iam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "botocore.client.Redshift"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(redshift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['log-data/',\n",
       " 'log-data/2018/11/2018-11-01-events.json',\n",
       " 'log-data/2018/11/2018-11-02-events.json',\n",
       " 'log-data/2018/11/2018-11-03-events.json',\n",
       " 'log-data/2018/11/2018-11-04-events.json',\n",
       " 'log-data/2018/11/2018-11-05-events.json',\n",
       " 'log-data/2018/11/2018-11-06-events.json',\n",
       " 'log-data/2018/11/2018-11-07-events.json',\n",
       " 'log-data/2018/11/2018-11-08-events.json',\n",
       " 'log-data/2018/11/2018-11-09-events.json']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Connect to a S3 bucket in aws vpc through BOTO3 object S3 by creating bucket object.\n",
    "bucket=s3.Bucket('udacity-dend')\n",
    "\n",
    "\n",
    "## Access the log data files in the s3 bucket 'udacity-dend' , and filter the files based on the prefix.\n",
    "log_data_files = [filename.key for filename in bucket.objects.filter(Prefix='log-data')]\n",
    "log_data_files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['song-data/A/A/A/TRAAAAK128F9318786.json',\n",
       " 'song-data/A/A/A/TRAAAAV128F421A322.json',\n",
       " 'song-data/A/A/A/TRAAABD128F429CF47.json',\n",
       " 'song-data/A/A/A/TRAAACN128F9355673.json',\n",
       " 'song-data/A/A/A/TRAAAEA128F935A30D.json',\n",
       " 'song-data/A/A/A/TRAAAED128E0783FAB.json',\n",
       " 'song-data/A/A/A/TRAAAEM128F93347B9.json',\n",
       " 'song-data/A/A/A/TRAAAEW128F42930C0.json',\n",
       " 'song-data/A/A/A/TRAAAFD128F92F423A.json',\n",
       " 'song-data/A/A/A/TRAAAGR128F425B14B.json']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Access the song data files in the s3 bucket 'udacity-dend', and filter the files based on the prefix.\n",
    "song_data_files = [filename.key for filename in bucket.objects.filter(Prefix='song-data/A')]\n",
    "song_data_files[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establishing connection with the Redshift DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Variables for redshift DB connectivity from config file.\n",
    "LOG_DATA          = config.get(\"S3\",\"LOG_DATA\")\n",
    "LOG_PATH          = config.get(\"S3\",\"LOG_JSONPATH\")\n",
    "SONG_DATA         = config.get(\"S3\",\"SONG_DATA\")\n",
    "IAM_ROLE          = config.get(\"IAM_ROLE\",\"ARN\")\n",
    "HOST              = config.get(\"CLUSTER\",\"HOST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connection between python and redshift can be done in multiple ways:\n",
    "\n",
    "#### 1. Using psycopg2:  https://www.blendo.co/blog/access-your-data-in-amazon-redshift-and-postgresql-with-python-and-r/\n",
    "\n",
    "#### 2. Using SQLAlchemy : \n",
    "\n",
    "#### 3. using simple SQL using URL as given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Format for calling theURL: postgresql://username:password@host:port/databasename\n",
    "## AWS Documentation for Copying Data to Redshift:  https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html#r_COPY-syntax-overview-data-source\n",
    "## https://docs.aws.amazon.com/redshift/latest/dg/t_Loading_tables_with_the_COPY_command.html\n",
    "conn_string = \"postgresql://{}:{}@{}:{}/{}\".format(DWH_DB_USER, DWH_DB_PASSWORD, HOST, DWH_PORT, DWH_DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql://dwhuser:Passw0rd@dwh-cluster.culrjdfmjjzn.ap-southeast-2.redshift.amazonaws.com:5439/dwh\n"
     ]
    }
   ],
   "source": [
    "print(conn_string) # Check the format in the print output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connected: dwhuser@dwh'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The output should be 'Connected: user_name@db_name'. \n",
    "## Important: It only gets connected when we run the object conn_string or call the get request using the above link.\n",
    "\n",
    "%sql $conn_string  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Staging Tables and Star Schema Tables to create a data warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwh-cluster.culrjdfmjjzn.ap-southeast-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      " * postgresql://dwhuser:***@dwh-cluster.culrjdfmjjzn.ap-southeast-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DROP TABLES if they already exist.\n",
    "\n",
    "staging_events_table_drop = \"DROP TABLE IF EXISTS staging_events\"   \n",
    "staging_songs_table_drop  = \"DROP TABLE IF EXISTS staging_songs\"\n",
    "\n",
    "%sql $staging_events_table_drop\n",
    "%sql $staging_songs_table_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREATE Staging TABLES: \n",
    "\n",
    "staging_events_table_create= (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS staging_events\n",
    "(\n",
    "artist          VARCHAR,\n",
    "auth            VARCHAR, \n",
    "firstName       VARCHAR,\n",
    "gender          VARCHAR,   \n",
    "itemInSession   INTEGER,\n",
    "lastName        VARCHAR,\n",
    "length          FLOAT,\n",
    "level           VARCHAR, \n",
    "location        VARCHAR,\n",
    "method          VARCHAR,\n",
    "page            VARCHAR,\n",
    "registration    BIGINT,\n",
    "sessionId       INTEGER,\n",
    "song            VARCHAR,\n",
    "status          INTEGER,\n",
    "ts              TIMESTAMP,\n",
    "userAgent       VARCHAR,\n",
    "userId          INTEGER\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "staging_songs_table_create = (\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS staging_songs\n",
    "(\n",
    "song_id            VARCHAR,\n",
    "num_songs          INTEGER,\n",
    "title              VARCHAR,\n",
    "artist_name        VARCHAR,\n",
    "artist_latitude    FLOAT,\n",
    "year               INTEGER,\n",
    "duration           FLOAT,\n",
    "artist_id          VARCHAR,\n",
    "artist_longitude   FLOAT,\n",
    "artist_location    VARCHAR\n",
    ");\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwh-cluster.culrjdfmjjzn.ap-southeast-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      " * postgresql://dwhuser:***@dwh-cluster.culrjdfmjjzn.ap-southeast-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql $staging_events_table_create\n",
    "%sql $staging_songs_table_create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Staging Tables into the redshift Database by using COPY command.\n",
    "### AWS Documentation for Copying Data to Redshift:  https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html#r_COPY-syntax-overview-data-source\n",
    "\n",
    "### https://docs.aws.amazon.com/redshift/latest/dg/t_loading-tables-from-s3.html\n",
    "\n",
    "### https://docs.aws.amazon.com/redshift/latest/dg/tutorial-loading-data.html\n",
    "\n",
    "### The staging table is be used to create the data-warehouse (here we are creating only one datamart in data warehouse) having a star schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGING TABLES\n",
    "\n",
    "## Remember the S3 buckets we are calling here is in US.\n",
    "\n",
    "staging_events_copy = (\"\"\"\n",
    "    COPY staging_events FROM {}\n",
    "    CREDENTIALS 'aws_iam_role={}'\n",
    "    COMPUPDATE OFF region 'us-west-2'\n",
    "    TIMEFORMAT as 'epochmillisecs'\n",
    "    TRUNCATECOLUMNS BLANKSASNULL EMPTYASNULL\n",
    "    FORMAT AS JSON 'auto'\n",
    "\"\"\").format(LOG_DATA, IAM_ROLE)\n",
    "\n",
    "\n",
    "staging_songs_copy = (\"\"\"\n",
    "    COPY staging_songs FROM {}\n",
    "    CREDENTIALS 'aws_iam_role={}'\n",
    "    COMPUPDATE OFF region 'us-west-2'\n",
    "    FORMAT AS JSON 'auto' \n",
    "    TRUNCATECOLUMNS BLANKSASNULL EMPTYASNULL;\n",
    "\"\"\").format(SONG_DATA, IAM_ROLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwh-cluster.culrjdfmjjzn.ap-southeast-2.redshift.amazonaws.com:5439/dwh\n",
      "Done.\n",
      " * postgresql://dwhuser:***@dwh-cluster.culrjdfmjjzn.ap-southeast-2.redshift.amazonaws.com:5439/dwh\n"
     ]
    }
   ],
   "source": [
    "%sql $staging_events_copy\n",
    "%sql $staging_songs_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "select * from staging_songs limit 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "select * from staging_events limit 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
