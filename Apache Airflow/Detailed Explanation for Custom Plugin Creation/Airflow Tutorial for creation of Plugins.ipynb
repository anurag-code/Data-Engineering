{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plugin Class creation basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First we will see the code without Plugin\n",
    "2. Then we will create a custom Plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Without plugin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.contrib.hooks.aws_hook import AwsHook\n",
    "from airflow.hooks.postgres_hook import PostgresHook\n",
    "from airflow.operators.postgres_operator import PostgresOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "import sql_statements\n",
    "\n",
    "## function definition for the task for loading_trip_data_to_redshift\n",
    "def load_trip_data_to_redshift(*args, **kwargs):\n",
    "    aws_hook = AwsHook(\"aws_credentials\")\n",
    "    credentials = aws_hook.get_credentials()\n",
    "    redshift_hook = PostgresHook(\"redshift\")\n",
    "    execution_date = kwargs[\"execution_date\"]\n",
    "    sql_stmt = sql_statements.COPY_MONTHLY_TRIPS_SQL.format(\n",
    "        credentials.access_key,\n",
    "        credentials.secret_key,\n",
    "        year=execution_date.year,\n",
    "        month=execution_date.month\n",
    "    )\n",
    "    redshift_hook.run(sql_stmt)\n",
    "\n",
    "## function definition for the task for loading_station_data_to_redshift\n",
    "def load_station_data_to_redshift(*args, **kwargs):\n",
    "    aws_hook = AwsHook(\"aws_credentials\")\n",
    "    credentials = aws_hook.get_credentials()\n",
    "    redshift_hook = PostgresHook(\"redshift\")\n",
    "    sql_stmt = sql_statements.COPY_STATIONS_SQL.format(\n",
    "        credentials.access_key,\n",
    "        credentials.secret_key,\n",
    "    )\n",
    "    redshift_hook.run(sql_stmt)\n",
    "\n",
    "\n",
    "## function definition for the code quality task for checking_greater_than_zero  \n",
    "def check_greater_than_zero(*args, **kwargs):\n",
    "    table = kwargs[\"params\"][\"table\"]\n",
    "    redshift_hook = PostgresHook(\"redshift\")\n",
    "    records = redshift_hook.get_records(f\"SELECT COUNT(*) FROM {table}\")\n",
    "    if len(records) < 1 or len(records[0]) < 1:\n",
    "        raise ValueError(f\"Data quality check failed. {table} returned no results\")\n",
    "    num_records = records[0][0]\n",
    "    if num_records < 1:\n",
    "        raise ValueError(f\"Data quality check failed. {table} contained 0 rows\")\n",
    "    logging.info(f\"Data quality on table {table} check passed with {records[0][0]} records\")\n",
    "\n",
    "\n",
    "## Dag definition\n",
    "dag = DAG(\n",
    "    'lesson2.exercise4',\n",
    "    start_date=datetime.datetime(2018, 1, 1, 0, 0, 0, 0),\n",
    "    end_date=datetime.datetime(2018, 12, 1, 0, 0, 0, 0),\n",
    "    schedule_interval='@monthly',\n",
    "    max_active_runs=1\n",
    ")\n",
    "\n",
    "\n",
    "## Task to create the trips table in redshift\n",
    "create_trips_table = PostgresOperator(\n",
    "    task_id=\"create_trips_table\",\n",
    "    dag=dag,\n",
    "    postgres_conn_id=\"redshift\",\n",
    "    sql=sql_statements.CREATE_TRIPS_TABLE_SQL\n",
    ")\n",
    "\n",
    "\n",
    "## Task to load_trips_data_from_s3_to_redshift\n",
    "copy_trips_task = PythonOperator(\n",
    "    task_id='load_trips_from_s3_to_redshift',\n",
    "    dag=dag,\n",
    "    python_callable=load_trip_data_to_redshift,\n",
    "    provide_context=True,\n",
    ")\n",
    "\n",
    "## Task to check data quality\n",
    "check_trips = PythonOperator(\n",
    "    task_id='check_trips_data',\n",
    "    dag=dag,\n",
    "    python_callable=check_greater_than_zero,\n",
    "    provide_context=True,\n",
    "    params={\n",
    "        'table': 'trips',\n",
    "    }\n",
    ")\n",
    "\n",
    "## Task to create_stations_table\n",
    "create_stations_table = PostgresOperator(\n",
    "    task_id=\"create_stations_table\",\n",
    "    dag=dag,\n",
    "    postgres_conn_id=\"redshift\",\n",
    "    sql=sql_statements.CREATE_STATIONS_TABLE_SQL,\n",
    ")\n",
    "\n",
    "\n",
    "## Task to load_stations_data_from_s3_to_redshift\n",
    "copy_stations_task = PythonOperator(\n",
    "    task_id='load_stations_from_s3_to_redshift',\n",
    "    dag=dag,\n",
    "    python_callable=load_station_data_to_redshift,\n",
    ")\n",
    "\n",
    "\n",
    "## Task to check_stations_data \n",
    "check_stations = PythonOperator(\n",
    "    task_id='check_stations_data',\n",
    "    dag=dag,\n",
    "    python_callable=check_greater_than_zero,\n",
    "    provide_context=True,\n",
    "    params={\n",
    "        'table': 'stations',\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "## dependencies\n",
    "create_trips_table >> copy_trips_task\n",
    "create_stations_table >> copy_stations_task\n",
    "copy_stations_task >> check_stations\n",
    "copy_trips_task >> check_trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE : We can see above that we have created a function with function name as 'check_greater_than_zero'; this function is being used multiple times, therefore, we can see that there is an opportunity that we can create an operator. This operator can be used instead of writing quality check function all the time while creating any pipeline. After creating the operator we just have to pass the parameters to the operator class to create a task.\n",
    "\n",
    "#### -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Plugin: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. has_rows plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### -----------------------------------------------------------------\n",
    "###### Before creating plugin we should Review Inheritance in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name is Rahul; id is 886012; salary is 1000000; post is BD\n"
     ]
    }
   ],
   "source": [
    "# Example Python code to demonstrate how parent constructors \n",
    "# are called. \n",
    "  \n",
    "# parent class \n",
    "class Person( object ):     \n",
    "  \n",
    "        # __init__ is known as the constructor          \n",
    "        def __init__(self, name, idnumber):    \n",
    "                self.name = name \n",
    "                self.idnumber = idnumber \n",
    "        def display(self): \n",
    "                print(self.name) \n",
    "                print(self.idnumber) \n",
    "  \n",
    "# child class  \n",
    "class Employee( Person ):            \n",
    "        def __init__(self, name, idnumber, salary, post): \n",
    "                self.salary = salary \n",
    "                self.post = post \n",
    "  \n",
    "                # invoking the __init__ of the parent class  \n",
    "                Person.__init__(self, name, idnumber)\n",
    "        \n",
    "        def show(self):\n",
    "            print(f\"name is {self.name}; id is {self.idnumber}; salary is {self.salary};\\\n",
    " post is {self.post}\")\n",
    "  \n",
    "                  \n",
    "# creation of an object variable or an instance \n",
    "a = Employee('Rahul', 886012, 1000000, \"BD\")     \n",
    "  \n",
    "# calling a function of the class Person using its instance \n",
    "a.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------------------------------------------------------\n",
    "\n",
    "##### Plugin contd....\n",
    "\n",
    "##### Core idea behind creating a plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. See the objects required for creating a class from the function definition.\n",
    " In our case we require: table name, redshift connection, and context variables args , kwargs\n",
    " \n",
    "2. Since we know nthe parameters we need ; so now we can create a class passing the above objects as a parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function definition for the code quality task for checking_greater_than_zero  \n",
    "def check_greater_than_zero(*args, **kwargs):\n",
    "    table = kwargs[\"params\"][\"table\"]\n",
    "    redshift_hook = PostgresHook(\"redshift\")\n",
    "    records = redshift_hook.get_records(f\"SELECT COUNT(*) FROM {table}\")\n",
    "    if len(records) < 1 or len(records[0]) < 1:\n",
    "        raise ValueError(f\"Data quality check failed. {table} returned no results\")\n",
    "    num_records = records[0][0]\n",
    "    if num_records < 1:\n",
    "        raise ValueError(f\"Data quality check failed. {table} contained 0 rows\")\n",
    "    logging.info(f\"Data quality on table {table} check passed with {records[0][0]} records\")\n",
    "    \n",
    "    \n",
    "## From the function above require: table name, redshift connection, and context variables args , kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plugin Definition\n",
    "\n",
    "import logging # It is used to include information (generated by user) into the airflow logs. \n",
    "# Helpful in debugging the code\n",
    "\n",
    "from airflow.hooks.postgres_hook import PostgresHook  \n",
    "\n",
    "from airflow.models import BaseOperator # Base Operator : is a parent class inherited by all the plugins\n",
    "\n",
    "from airflow.utils.decorators import apply_defaults # It is used as a decorator function around the class constructor  \n",
    "\n",
    "\n",
    "class HasRowsOperator(BaseOperator):\n",
    "\n",
    "    @apply_defaults\n",
    "    def __init__(self, redshift_conn_id=\"\", table=\"\", *args, **kwargs):\n",
    "        self.table = table\n",
    "        self.redshift_conn_id = redshift_conn_id\n",
    "        \n",
    "        # invoking constructor of parent class\n",
    "        super(HasRowsOperator, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def execute(self, context):\n",
    "        redshift_hook = PostgresHook(self.redshift_conn_id)\n",
    "        records = redshift_hook.get_records(f\"SELECT COUNT(*) FROM {self.table}\")\n",
    "        if len(records) < 1 or len(records[0]) < 1:\n",
    "            raise ValueError(f\"Data quality check failed. {self.table} returned no results\")\n",
    "        num_records = records[0][0]\n",
    "        if num_records < 1:\n",
    "            raise ValueError(f\"Data quality check failed. {self.table} contained 0 rows\")\n",
    "        logging.info(f\"Data quality on table {self.table} check passed with {records[0][0]} records\")\n",
    "        \n",
    "# BaseOperator is a parent class and is a must for all the plugin creation as per airflow documentation.\n",
    "\n",
    "# @apply_defaults: is adecorator function. Its usage is to be researched.\n",
    "\n",
    "# def __init__(self, redshift_conn_id=\"\", table=\"\", *args, **kwargs):\n",
    "#  pass the table name, redshift connection, and context variables to the \n",
    "# - constructor function of the HasRowsOperator() class.\n",
    "\n",
    "# super(HasRowsOperator, self).__init__(*args, **kwargs) : is meant to invoke the constructor function of the parent calss\n",
    "\n",
    "# def execute(self, context): This function contains all the operations which were there \n",
    "# - in the function definition of check_greater_than_zero(*args, **kwargs).\n",
    "# - This function is a must for the operator creation. At the time of the run of this operator in a dag, the execute function is executed by the  airflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This basic template is used to create any custom plugin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('base': conda)",
   "language": "python",
   "name": "python37064bitbasecondac55358e60d5c4982ad0344776e7ad4ad"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
