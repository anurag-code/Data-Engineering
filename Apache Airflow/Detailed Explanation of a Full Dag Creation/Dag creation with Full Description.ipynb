{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of a Full Dag\n",
    "### Part 1. Full code\n",
    "### Part 2. Detailed Explanation of the full code.\n",
    "\n",
    "#### -----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1. Full code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.contrib.hooks.aws_hook import AwsHook\n",
    "from airflow.hooks.postgres_hook import PostgresHook\n",
    "from airflow.operators.postgres_operator import PostgresOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "import sql_statements\n",
    "\n",
    "## function definition for the task for loading_trip_data_to_redshift\n",
    "def load_trip_data_to_redshift(*args, **kwargs):\n",
    "    aws_hook = AwsHook(\"aws_credentials\")\n",
    "    credentials = aws_hook.get_credentials()\n",
    "    redshift_hook = PostgresHook(\"redshift\")\n",
    "    execution_date = kwargs[\"execution_date\"]\n",
    "    sql_stmt = sql_statements.COPY_MONTHLY_TRIPS_SQL.format(\n",
    "        credentials.access_key,\n",
    "        credentials.secret_key,\n",
    "        year=execution_date.year,\n",
    "        month=execution_date.month\n",
    "    )\n",
    "    redshift_hook.run(sql_stmt)\n",
    "\n",
    "## function definition for the task for loading_station_data_to_redshift\n",
    "def load_station_data_to_redshift(*args, **kwargs):\n",
    "    aws_hook = AwsHook(\"aws_credentials\")\n",
    "    credentials = aws_hook.get_credentials()\n",
    "    redshift_hook = PostgresHook(\"redshift\")\n",
    "    sql_stmt = sql_statements.COPY_STATIONS_SQL.format(\n",
    "        credentials.access_key,\n",
    "        credentials.secret_key,\n",
    "    )\n",
    "    redshift_hook.run(sql_stmt)\n",
    "\n",
    "\n",
    "## function definition for the code quality task for checking_greater_than_zero  \n",
    "def check_greater_than_zero(*args, **kwargs):\n",
    "    table = kwargs[\"params\"][\"table\"]\n",
    "    redshift_hook = PostgresHook(\"redshift\")\n",
    "    records = redshift_hook.get_records(f\"SELECT COUNT(*) FROM {table}\")\n",
    "    if len(records) < 1 or len(records[0]) < 1:\n",
    "        raise ValueError(f\"Data quality check failed. {table} returned no results\")\n",
    "    num_records = records[0][0]\n",
    "    if num_records < 1:\n",
    "        raise ValueError(f\"Data quality check failed. {table} contained 0 rows\")\n",
    "    logging.info(f\"Data quality on table {table} check passed with {records[0][0]} records\")\n",
    "\n",
    "\n",
    "## Dag definition\n",
    "dag = DAG(\n",
    "    'lesson2.exercise4',\n",
    "    start_date=datetime.datetime(2018, 1, 1, 0, 0, 0, 0),\n",
    "    end_date=datetime.datetime(2018, 12, 1, 0, 0, 0, 0),\n",
    "    schedule_interval='@monthly',\n",
    "    max_active_runs=1\n",
    ")\n",
    "\n",
    "\n",
    "## Task to create the trips table in redshift\n",
    "create_trips_table = PostgresOperator(\n",
    "    task_id=\"create_trips_table\",\n",
    "    dag=dag,\n",
    "    postgres_conn_id=\"redshift\",\n",
    "    sql=sql_statements.CREATE_TRIPS_TABLE_SQL\n",
    ")\n",
    "\n",
    "\n",
    "## Task to load_trips_data_from_s3_to_redshift\n",
    "copy_trips_task = PythonOperator(\n",
    "    task_id='load_trips_from_s3_to_redshift',\n",
    "    dag=dag,\n",
    "    python_callable=load_trip_data_to_redshift,\n",
    "    provide_context=True,\n",
    ")\n",
    "\n",
    "## Task to check data quality\n",
    "check_trips = PythonOperator(\n",
    "    task_id='check_trips_data',\n",
    "    dag=dag,\n",
    "    python_callable=check_greater_than_zero,\n",
    "    provide_context=True,\n",
    "    params={\n",
    "        'table': 'trips',\n",
    "    }\n",
    ")\n",
    "\n",
    "## Task to create_stations_table\n",
    "create_stations_table = PostgresOperator(\n",
    "    task_id=\"create_stations_table\",\n",
    "    dag=dag,\n",
    "    postgres_conn_id=\"redshift\",\n",
    "    sql=sql_statements.CREATE_STATIONS_TABLE_SQL,\n",
    ")\n",
    "\n",
    "\n",
    "## Task to load_stations_data_from_s3_to_redshift\n",
    "copy_stations_task = PythonOperator(\n",
    "    task_id='load_stations_from_s3_to_redshift',\n",
    "    dag=dag,\n",
    "    python_callable=load_station_data_to_redshift,\n",
    ")\n",
    "\n",
    "\n",
    "## Task to check_stations_data \n",
    "check_stations = PythonOperator(\n",
    "    task_id='check_stations_data',\n",
    "    dag=dag,\n",
    "    python_callable=check_greater_than_zero,\n",
    "    provide_context=True,\n",
    "    params={\n",
    "        'table': 'stations',\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "## dependencies\n",
    "create_trips_table >> copy_trips_task\n",
    "create_stations_table >> copy_stations_task\n",
    "copy_stations_task >> check_stations\n",
    "copy_trips_task >> check_trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Detailed Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime  # Used for datetime calculations.\n",
    "\n",
    "\n",
    "import logging   # It is used to include information (generated by user) into the airflow logs. \n",
    "# Helpful in debugging the code\n",
    "\n",
    "\n",
    "from airflow import DAG  # This Class helps us to generate dags.\n",
    "\n",
    "\n",
    "# Hooks:\n",
    "# Hooks are basically used to connect to other applications through their APIs. \n",
    "# For instance we can connect Airflow application to AWS, Azure, GCP , redshift , Gcp storage, S3 etc.\n",
    "# We can also use the associated methods/functions related to the hooks.\n",
    "# There are two types of hooks: 1. created by Apache Airflow ; 2. Hooks created and contributed by the users.\n",
    "\n",
    "from airflow.contrib.hooks.aws_hook import AwsHook #  This is meant to interact with AWS. \n",
    "# This class is a thin wrapper around the boto3 python library.\n",
    "# The airflow contributors have created several custom hooks.\n",
    "# These hooks have been shared and opensourced for all the airflow users. Similarly there are several \n",
    "# - custom operators created by airflow contributors. \n",
    "# We can check available hooks and operators as mentioned below:\n",
    "# hooks     :   https://airflow.apache.org/docs/stable/_api/airflow/contrib/hooks/index.html\n",
    "# operators :   https://airflow.apache.org/docs/stable/_api/airflow/contrib/operators/index.html\n",
    "\n",
    "    \n",
    "\n",
    "from airflow.hooks.postgres_hook import PostgresHook # Postgrehook is a hook created by Apache Airflow.\n",
    "# It can be used to connect postgres database as well as AWS redshift.\n",
    "\n",
    "\n",
    "# Operators: \n",
    "# While DAGs describe how to run a workflow, Operators determine what actually gets done.\n",
    "# Operators are a class which when instantiated with params then it becomes a task.\n",
    "# An operator describes a single task in a workflow. Operators are usually (but not always) atomic, \n",
    "# - meaning they can stand on their own and donâ€™t need to share resources with any other operators. \n",
    "# The DAG will make sure that operators run in the correct certain order; other than those dependencies, \n",
    "# - operators generally run independently. In fact, they may run on two completely different machines.\n",
    "\n",
    "from airflow.operators.postgres_operator import PostgresOperator # This operator is built by Apache Airflow itself.\n",
    "# This is used to create tasks, which deals not only  with postgresql, but also AWS redshift.\n",
    "\n",
    "from airflow.operators.python_operator import PythonOperator # Python operator is used to create python related tasks.\n",
    "\n",
    "# Note: Since we know that Apapche airflow is written in python, so why are using operators other than pythonoperator?\n",
    "# Yes , we can do it solely with python operator but there are several operator classes \n",
    "# - that have been prebuilt using python to assist us in creaing the tasks efficiently\n",
    "# - with less coding. It helps in reducing the repeatbale tasks in airflow by creating custom classes\n",
    "# - and associated methods for making our coding efficient. That is also the reason\n",
    "# - why airflow community is constantly contributing custom operators and hooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sql_statements  # sql statements have been written in this python file and has been imported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAG Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag = DAG(\n",
    "          'Dag_Name_abc',\n",
    "           start_date=datetime.datetime(2018, 1, 1, 0, 0, 0, 0),\n",
    "           end_date=datetime.datetime(2018, 12, 1, 0, 0, 0, 0),\n",
    "           schedule_interval='@monthly',\n",
    "           max_active_runs= 1\n",
    "         )\n",
    "\n",
    "# start_date :datetime.datetime(year, month, day, hour, minutes, seconds, microSeconds); start date of dag\n",
    "\n",
    "# end_date   :datetime.datetime(year, month, day, hour, minutes, seconds, microSeconds); end date of dag\n",
    "\n",
    "# schedule_interval: Interval at which the dag will run between start and end date.\n",
    "\n",
    "# max_active_run : During Backfilling Jobs, if we want our dags to run one after another only then we assign\n",
    "\n",
    "# - its value as 1. This happens when we have more than one workers to run our dags in parallel.\n",
    "\n",
    "# Note: Backfilling means that when our start date is less than our current date then  \n",
    "# - airflow tries to backfill all the dag runs which were pending."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task related Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function definition for the task for loading_trip_data_to_redshift\n",
    "def load_trip_data_to_redshift(*args, **kwargs):\n",
    "    aws_hook = AwsHook(\"aws_credentials\")\n",
    "    credentials = aws_hook.get_credentials()\n",
    "    redshift_hook = PostgresHook(\"redshift\")\n",
    "    execution_date = kwargs[\"execution_date\"]\n",
    "    sql_stmt = sql_statements.COPY_MONTHLY_TRIPS_SQL.format(\n",
    "        credentials.access_key,\n",
    "        credentials.secret_key,\n",
    "        year=execution_date.year,\n",
    "        month=execution_date.month\n",
    "    )\n",
    "    redshift_hook.run(sql_stmt)\n",
    "    \n",
    "\n",
    "# *args and **kwargs: are default variables, which can be accessed by the functions  \n",
    "# -related to an operator only when in the operator definition we state that 'provide_context=True'.\n",
    "\n",
    "# AwsHook(\"aws_credentials\"): is a hook imported above. We have passed \"aws_credentials\" to AwsHook.\n",
    "# \"aws_credentials\" is the name of the credentials data (aws secret key, aws access key etc) \n",
    "# - fed into the connections using connection tab.\n",
    "\n",
    "# aws_hook.get_credentials() : get_credentials() is a method available for aws_hook instance.\n",
    "# This is ment to Get the underlying `botocore.Credentials` object.\n",
    "# This contains the following authentication attributes: access_key, secret_key and token \n",
    "# - which can be acessed by dot operator (credentials.access_key, credentials.secret_key).\n",
    "\n",
    "# PostgresHook(\"redshift\"): Postgresql hook is meant for connecting to AWS redshift.  \n",
    "# - Here \"redshift\" is the name of the connection created in the connection tab.\n",
    "\n",
    "# kwargs[\"execution_date\"]: Here \"execution_date\" is the default context variable \n",
    "# - available for the operator with 'provide_context=True'.\n",
    "# - This returns an object with two attributes year , month. \n",
    "# A very good article to understand this is \n",
    "# -  https://blog.godatadriven.com/zen-of-python-and-apache-airflow\n",
    "\n",
    "# sql_stmt: description is given in below cell\n",
    "\n",
    "#  redshift_hook.run(sql_stmt): We can run our sql query using the run() method \n",
    "# - on the object redshift_hook created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql_stmt:\n",
    "# Below mentioned query is imported from 'sql_statements.py'\n",
    "# sql_stmt: The sql_stmt above is the formatted SQL query to which \n",
    "# - we are passing the params to the SQL query.\n",
    "# COPY_SQL: the query written here is the format in which aws sql query command \n",
    "# - for copy is written when copying data to redshift from S3.\n",
    "\n",
    "\n",
    "COPY_SQL = \"\"\"\n",
    "COPY {}\n",
    "FROM '{}'\n",
    "ACCESS_KEY_ID '{{}}'\n",
    "SECRET_ACCESS_KEY '{{}}'\n",
    "IGNOREHEADER 1\n",
    "DELIMITER ','\n",
    "\"\"\"\n",
    "\n",
    "COPY_MONTHLY_TRIPS_SQL = COPY_SQL.format(\n",
    "    \"trips\",\n",
    "    \"s3://udacity-dend/data-pipelines/divvy/partitioned/{year}/{month}/divvy_trips.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keyword arguments available when using 'provide_context=True' , with some random sample values:\n",
    "\n",
    "# {\n",
    "# 'END_DATE': '2019-01-01',\n",
    "# 'conf': <module 'airflow.configuration' from '/opt/conda/lib/python3.6/site-packages/airflow/configuration.py'>,\n",
    "# 'dag': <DAG: context_demo>,\n",
    "# 'dag_run': None,\n",
    "# 'ds': '2019-01-01',\n",
    "# 'ds_nodash': '20190101',\n",
    "# 'end_date': '2019-01-01',\n",
    "# 'execution_date': <Pendulum [2019-01-01T00:00:00+00:00]>,\n",
    "# 'inlets': [],\n",
    "# 'latest_date': '2019-01-01',\n",
    "# 'macros': <module 'airflow.macros' from '/opt/conda/lib/python3.6/site-packages/airflow/macros/__init__.py'>,\n",
    "# 'next_ds': '2019-01-02',\n",
    "# 'next_ds_nodash': '20190102',\n",
    "# 'next_execution_date': datetime.datetime(2019, 1, 2, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>),\n",
    "# 'outlets': [],\n",
    "# 'params': {},\n",
    "# 'prev_ds': '2018-12-31',\n",
    "# 'prev_ds_nodash': '20181231',\n",
    "# 'prev_execution_date': datetime.datetime(2018, 12, 31, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>),\n",
    "# 'run_id': None,\n",
    "# 'tables': None,\n",
    "# 'task': <Task(PythonOperator): print_exec_date>,\n",
    "# 'task_instance': <TaskInstance: context_demo.print_exec_date 2019-01-01T00:00:00+00:00 [None]>,\n",
    "# 'task_instance_key_str': 'context_demo__print_exec_date__20190101',\n",
    "# 'templates_dict': None,\n",
    "# 'test_mode': True,\n",
    "# 'ti': <TaskInstance: context_demo.print_exec_date 2019-01-01T00:00:00+00:00 [None]>,\n",
    "# 'tomorrow_ds': '2019-01-02',\n",
    "# 'tomorrow_ds_nodash': '20190102',\n",
    "# 'ts': '2019-01-01T00:00:00+00:00',\n",
    "# 'ts_nodash': '20190101T000000',\n",
    "# 'ts_nodash_with_tz': '20190101T000000+0000',\n",
    "# 'var': {'json': None, 'value': None},\n",
    "# 'yesterday_ds': '2018-12-31',\n",
    "# 'yesterday_ds_nodash': '20181231'\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "#### Note: We can pass more parameters value in a dictionary format to the key word 'params' above in the the operator definition directly. We will see that example in below codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function is very similar to the above function. So no explanation needed in this case.\n",
    "\n",
    "## function definition for the task for loading_station_data_to_redshift\n",
    "def load_station_data_to_redshift(*args, **kwargs):\n",
    "    aws_hook = AwsHook(\"aws_credentials\")\n",
    "    credentials = aws_hook.get_credentials()\n",
    "    redshift_hook = PostgresHook(\"redshift\")\n",
    "    sql_stmt = sql_statements.COPY_STATIONS_SQL.format(\n",
    "        credentials.access_key,\n",
    "        credentials.secret_key,\n",
    "    )\n",
    "    redshift_hook.run(sql_stmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Quality Checks Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function definition for the code quality task for checking_greater_than_zero  \n",
    "def check_greater_than_zero(*args, **kwargs):\n",
    "    table = kwargs[\"params\"][\"table\"]\n",
    "    redshift_hook = PostgresHook(\"redshift\")\n",
    "    records = redshift_hook.get_records(f\"SELECT COUNT(*) FROM {table}\")\n",
    "    if len(records) < 1 or len(records[0]) < 1:\n",
    "        raise ValueError(f\"Data quality check failed. {table} returned no results\")\n",
    "    num_records = records[0][0]\n",
    "    if num_records < 1:\n",
    "        raise ValueError(f\"Data quality check failed. {table} contained 0 rows\")\n",
    "    logging.info(f\"Data quality on table {table} check passed with {records[0][0]} records\")\n",
    "    \n",
    "    \n",
    "# kwargs[\"params\"][\"table\"]: As mentioned above , we have passed a dictionary for table \n",
    "# - name in the keyword 'params' of the context variables. Thus to get the table \n",
    "# - name we are calling a key of dictionary stored within a  larger dictionary kwargs. \n",
    "\n",
    "\n",
    "# PostgresHook(\"redshift\"): We have already defined above. \n",
    "\n",
    "# redshift_hook.get_records(f\"SELECT COUNT(*) FROM {table}\") : The method get_records(f\"SELECT COUNT(*) FROM {table}\")\n",
    "# - gives us the number of records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PostgreSql Operator and Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task to create the trips table in redshift\n",
    "create_trips_table = PostgresOperator(\n",
    "    task_id=\"create_trips_table\",\n",
    "    dag=dag,\n",
    "    postgres_conn_id=\"redshift\",\n",
    "    sql=sql_statements.CREATE_TRIPS_TABLE_SQL\n",
    ")\n",
    "\n",
    "# Instantiate the PostgresOperator with following parameters:\n",
    "\n",
    "# task_id : task name\n",
    "\n",
    "# dag: name of the dag with which this task is connected to.\n",
    "\n",
    "# postgres_conn_id: Name of the connection created through connection tab of airflow web UI. I\n",
    "# t contains Credentials required to connect redshift with the airflow.\n",
    "\n",
    "# sql: is the sql query which is to be ran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task to create_stations_table\n",
    "create_stations_table = PostgresOperator(\n",
    "    task_id=\"create_stations_table\",\n",
    "    dag=dag,\n",
    "    postgres_conn_id=\"redshift\",\n",
    "    sql=sql_statements.CREATE_STATIONS_TABLE_SQL,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Operator and Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task to load_trips_data_from_s3_to_redshift\n",
    "copy_trips_task = PythonOperator(\n",
    "    task_id='load_trips_from_s3_to_redshift',\n",
    "    dag=dag,\n",
    "    python_callable=load_trip_data_to_redshift,\n",
    "    provide_context=True,\n",
    ")\n",
    "\n",
    "# Instantiate the PythonOperator with following parameters:\n",
    "\n",
    "# task_id : task name\n",
    "\n",
    "# dag : name of the dag with which this task is connected to.\n",
    "\n",
    "# python_callable: name of the function which is the part of this task and needed to be called for the task.\n",
    "\n",
    "# provide_context : If this is true then we can get all the context variables accessible in the form of **kwargs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task to check data quality\n",
    "check_trips = PythonOperator(\n",
    "    task_id='check_trips_data',\n",
    "    dag=dag,\n",
    "    python_callable=check_greater_than_zero,\n",
    "    provide_context=True,\n",
    "    params={\n",
    "        'table': 'trips',\n",
    "    }\n",
    ")\n",
    "\n",
    "# Instantiate the PythonOperator with following parameters:\n",
    "\n",
    "# task_id : task name\n",
    "\n",
    "# dag : name of the dag with which this task is connected to.\n",
    "\n",
    "# python_callable: name of the function which is the part of this task and needed to be called for the task.\n",
    "\n",
    "# provide_context : If this is true then we can get all the context variables accessible in the form of **kwargs.\n",
    "\n",
    "# params: Once provide_context = True , then we can add some more parameters  \n",
    "# - in dictionary format  to the **kwargs (context variables) \n",
    "# - using params. Here we are passing a dictioanry {'table': 'trips'}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation is same as the above Python Operators\n",
    "\n",
    "## Task to load_stations_data_from_s3_to_redshift\n",
    "copy_stations_task = PythonOperator(\n",
    "    task_id='load_stations_from_s3_to_redshift',\n",
    "    dag=dag,\n",
    "    python_callable=load_station_data_to_redshift,\n",
    ")\n",
    "\n",
    "\n",
    "## Task to check_stations_data \n",
    "check_stations = PythonOperator(\n",
    "    task_id='check_stations_data',\n",
    "    dag=dag,\n",
    "    python_callable=check_greater_than_zero,\n",
    "    provide_context=True,\n",
    "    params={\n",
    "        'table': 'stations',\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
