{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["spark = SparkSession.builder.appName(\"basic examples1\").getOrCreate()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"markdown","source":["##### Schema on read: We can either let a data source define the schema (called schema-on-read) or we can define it explicitly ourselves."],"metadata":{}},{"cell_type":"code","source":["## read data from json\n\ndf =  spark.read.format(\"json\").load(\"/FileStore/tables/2015_summary-ebaee.json\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["df.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- DEST_COUNTRY_NAME: string (nullable = true)\n-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n-- count: long (nullable = true)\n\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["df.toPandas().head()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>DEST_COUNTRY_NAME</th>\n      <th>ORIGIN_COUNTRY_NAME</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>United States</td>\n      <td>Romania</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>United States</td>\n      <td>Croatia</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>United States</td>\n      <td>Ireland</td>\n      <td>344</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Egypt</td>\n      <td>United States</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>United States</td>\n      <td>India</td>\n      <td>62</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["print(df.schema)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,LongType,true)))\n</div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["#### The example that follows shows how to create and enforce a specific schema on a DataFrame."],"metadata":{}},{"cell_type":"code","source":["## import the data types \n\nfrom pyspark.sql.types import StructField, StructType, StringType, LongType\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["**A schema is a StructType made up of a number of fields -StructFields- that have :**\n\n1. **name of column**, \n2. **data type of that column**, \n3. **Boolean flag which specifies whether that column can contain missing or null values**,\nand,\n4. **finally, users can optionally specify associated metadata with that column**. The metadata is a way of storing information about this column (Spark uses this in its machine learning library)."],"metadata":{}},{"cell_type":"code","source":["myManualSchema = StructType([StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n                            StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n                            StructField(\"count\", LongType(), False, metadata={\"hello\":\"world\"})])\n\n## Here metadata is given just to show , how it is declared along with the other column info."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"code","source":["df = spark.read.format(\"json\").schema(myManualSchema).load(\"/FileStore/tables/2015_summary-ebaee.json\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["### Columns"],"metadata":{}},{"cell_type":"markdown","source":["To Spark, columns are logical constructions that simply represent a value computed on a perrecord\nbasis by means of an expression. This means that to have a real value for a column, we\nneed to have a row; and to have a row, we need to have a DataFrame. You cannot manipulate an\nindividual column outside the context of a DataFrame; you must use Spark transformations\nwithin a DataFrame to modify the contents of a column."],"metadata":{}},{"cell_type":"markdown","source":["#### Columns\n\nThere are a lot of different ways to construct and refer to columns but the two simplest ways are\nby using the col or column functions. To use either of these functions, you pass in a column\nname:\n\n**IMP NOTE: col and column functions are useful only in scala. in pyspark we use df.column_name or df[\"column_name\"] to reference a column**"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col, column\n\n# df.col(\"someColumnName\") \n# df.column(\"someColumnName\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16},{"cell_type":"code","source":["df.DEST_COUNTRY_NAME"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[11]: Column&lt;b&#39;DEST_COUNTRY_NAME&#39;&gt;</div>"]}}],"execution_count":17},{"cell_type":"code","source":["df[\"DEST_COUNTRY_NAME\"]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[12]: Column&lt;b&#39;DEST_COUNTRY_NAME&#39;&gt;</div>"]}}],"execution_count":18},{"cell_type":"code","source":["## to access the column names programatically \n\ndf.columns"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[13]: [&#39;DEST_COUNTRY_NAME&#39;, &#39;ORIGIN_COUNTRY_NAME&#39;, &#39;count&#39;]</div>"]}}],"execution_count":19},{"cell_type":"markdown","source":["#### Records and Rows"],"metadata":{}},{"cell_type":"markdown","source":["In Spark, each row in a DataFrame is a single record. **Spark represents this record as an object of\ntype Row.** Spark manipulates Row objects using column expressions in order to produce usable\nvalues. **Row objects internally represent arrays of bytes.** The byte array interface is never shown\nto users because we only use column expressions to manipulate them.\nYou’ll notice commands that return individual rows to the driver will always return one or more\nRow types when we are working with DataFrames."],"metadata":{}},{"cell_type":"code","source":["# Let’s see a row by calling first on our DataFrame:\n## Note that it returns a row type object but not a dataframe object.\n\ndf.first()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[14]: Row(DEST_COUNTRY_NAME=&#39;United States&#39;, ORIGIN_COUNTRY_NAME=&#39;Romania&#39;, count=15)</div>"]}}],"execution_count":22},{"cell_type":"markdown","source":["#### Create Rows"],"metadata":{}},{"cell_type":"markdown","source":["*You can create rows by manually instantiating a Row object with the values that belong in each\ncolumn. It’s important to note that only DataFrames have schemas. Rows themselves do not have\nschemas. This means that if you create a Row manually, you must specify the values in the same\norder as the schema of the DataFrame to which they might be appended*"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import Row\n\nmyRow = Row(\"Hello\", None, 1)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":25},{"cell_type":"markdown","source":["#### Create Dataframe\n\nWe will create an example DataFrame (for\nillustration purposes later in this chapter, **we will also register this as a temporary view so that we\ncan query it with SQL and show off basic transformations in SQL, as well).**"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import Row\n\nfrom pyspark.sql.types import StructField, StructType, StringType, LongType\n\nmyManualSchema = StructType([StructField(\"Some\", StringType(), True), StructField(\"Col\",StringType(),True),StructField(\"Names\",LongType(),False)])\n\nmyRow1= Row(\"Hello\", None, 34)\nmyRow2= Row(\"World\", \"yaa\", 45)\n\nmyDf = spark.createDataFrame([myRow1,myRow2],myManualSchema)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":27},{"cell_type":"code","source":["myDf.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+----+-----+\n Some| Col|Names|\n+-----+----+-----+\nHello|null|   34|\nWorld| yaa|   45|\n+-----+----+-----+\n\n</div>"]}}],"execution_count":28},{"cell_type":"code","source":["## create temporary table to run sql queries\nmyDf.createOrReplaceTempView(\"myDfTable\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":29},{"cell_type":"code","source":["spark.sql(\"select * from myDfTable\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+----+-----+\n Some| Col|Names|\n+-----+----+-----+\nHello|null|   34|\nWorld| yaa|   45|\n+-----+----+-----+\n\n</div>"]}}],"execution_count":30},{"cell_type":"markdown","source":["##### select and selectExpr"],"metadata":{}},{"cell_type":"code","source":["df.select(\"count\").show(5)\n\n## Equivalent of this in sql is --->  SELECT count FROM dfTable LIMIT 5"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+\ncount|\n+-----+\n   15|\n    1|\n  344|\n   15|\n   62|\n+-----+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":32},{"cell_type":"code","source":["df.select(\"count\", \"ORIGIN_COUNTRY_NAME\").show(4)\n\n## sql :  SELECT count, ORIGIN_COUNTRY_NAME FROM dfTable LIMIT 4;"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+-------------------+\ncount|ORIGIN_COUNTRY_NAME|\n+-----+-------------------+\n   15|            Romania|\n    1|            Croatia|\n  344|            Ireland|\n   15|      United States|\n+-----+-------------------+\nonly showing top 4 rows\n\n</div>"]}}],"execution_count":33},{"cell_type":"code","source":["## Select and expression\nfrom pyspark.sql.functions import expr\n\ndf.select(expr(\"ORIGIN_COUNTRY_NAME AS Origin\")).show(2)\n\n## sql: SELECT ORIGIN_COUNTRY_NAME AS destination FROM dfTable LIMIT 2"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+\n Origin|\n+-------+\nRomania|\nCroatia|\n+-------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":34},{"cell_type":"code","source":["## To change the name back to previous name , we can use alias.\ndf.select(expr(\"ORIGIN_COUNTRY_NAME as Origin\").alias(\"ORIGIN_COUNTRY_NAME\"))\\\n.show(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+\nORIGIN_COUNTRY_NAME|\n+-------------------+\n            Romania|\n            Croatia|\n+-------------------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":35},{"cell_type":"markdown","source":["Because select followed by a series of expr is such a common pattern, Spark has a shorthand\nfor doing this efficiently: **selectExpr**. This is probably the most convenient interface for\neveryday use:"],"metadata":{}},{"cell_type":"code","source":["## Usage of selectExpr()\n\ndf.selectExpr(\"ORIGIN_COUNTRY_NAME as Origin\", \"ORIGIN_COUNTRY_NAME\" )"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[24]: DataFrame[Origin: string, ORIGIN_COUNTRY_NAME: string]</div>"]}}],"execution_count":37},{"cell_type":"code","source":["# in Python\ndf.selectExpr(\"*\", \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\")\\\n.show(20)\n\n\n#-- in SQL\n# SELECT *, (DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry FROM dfTable LIMIT 20"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+-------------------+-----+-------------+\n   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n+--------------------+-------------------+-----+-------------+\n       United States|            Romania|   15|        false|\n       United States|            Croatia|    1|        false|\n       United States|            Ireland|  344|        false|\n               Egypt|      United States|   15|        false|\n       United States|              India|   62|        false|\n       United States|          Singapore|    1|        false|\n       United States|            Grenada|   62|        false|\n          Costa Rica|      United States|  588|        false|\n             Senegal|      United States|   40|        false|\n             Moldova|      United States|    1|        false|\n       United States|       Sint Maarten|  325|        false|\n       United States|   Marshall Islands|   39|        false|\n              Guyana|      United States|   64|        false|\n               Malta|      United States|    1|        false|\n            Anguilla|      United States|   41|        false|\n             Bolivia|      United States|   30|        false|\n       United States|           Paraguay|    6|        false|\n             Algeria|      United States|    4|        false|\nTurks and Caicos ...|      United States|  230|        false|\n       United States|          Gibraltar|    1|        false|\n+--------------------+-------------------+-----+-------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":38},{"cell_type":"code","source":["# in Python; Aggregation using selectExpr\ndf.selectExpr(\"avg(count)\", \"count(distinct(DEST_COUNTRY_NAME))\").show(2)\n\n#-- in SQL # SELECT avg(count), count(distinct(DEST_COUNTRY_NAME)) FROM dfTable LIMIT 2"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+---------------------------------+\n avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|\n+-----------+---------------------------------+\n1770.765625|                              132|\n+-----------+---------------------------------+\n\n</div>"]}}],"execution_count":39},{"cell_type":"markdown","source":["#### Literals\n\n**These are used to create a new constant column**. This will come up when you might need to check whether a value is greater than some constant\nor other programmatically created variable."],"metadata":{}},{"cell_type":"code","source":["# in Python\nfrom pyspark.sql.functions import lit\ndf.select(expr(\"*\"), lit(1).alias(\"One\")).show(2)\n\n# In SQL, literals are just the specific value:\n# -- in SQL\n# SELECT *, 1 as One FROM dfTable LIMIT 2"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-------------------+-----+---+\nDEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|\n+-----------------+-------------------+-----+---+\n    United States|            Romania|   15|  1|\n    United States|            Croatia|    1|  1|\n+-----------------+-------------------+-----+---+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":41},{"cell_type":"markdown","source":["#### New Column\n\nwithColumn() Function:\n\nNotice that the withColumn **function takes two arguments**: **the column name** and **the expression or a function**\nthat will create the value for that given row in the DataFrame."],"metadata":{}},{"cell_type":"code","source":["# in Python\ndf.withColumn(\"newColName\", lit(1)).show(2)\n#-- in SQL\n#SELECT *, 1 as newColName FROM dfTable LIMIT 2"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-------------------+-----+----------+\nDEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|newColName|\n+-----------------+-------------------+-----+----------+\n    United States|            Romania|   15|         1|\n    United States|            Croatia|    1|         1|\n+-----------------+-------------------+-----+----------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":43},{"cell_type":"code","source":["## Lets create a Boolean column\n\ndf.withColumn(\"withInCountry\", expr(\"DEST_COUNTRY_NAME==ORIGIN_COUNTRY_NAME\")).show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-------------------+-----+-------------+\nDEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withInCountry|\n+-----------------+-------------------+-----+-------------+\n    United States|            Romania|   15|        false|\n    United States|            Croatia|    1|        false|\n    United States|            Ireland|  344|        false|\n            Egypt|      United States|   15|        false|\n    United States|              India|   62|        false|\n+-----------------+-------------------+-----+-------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":44},{"cell_type":"markdown","source":["Interestingly, we can also rename\na column this way. The SQL syntax is the same as we had previously, so we can omit it in this\nexample:"],"metadata":{}},{"cell_type":"code","source":["df.withColumn(\"Destination\", expr(\"DEST_COUNTRY_NAME\")).columns"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[37]: [&#39;DEST_COUNTRY_NAME&#39;, &#39;ORIGIN_COUNTRY_NAME&#39;, &#39;count&#39;, &#39;Destination&#39;]</div>"]}}],"execution_count":46},{"cell_type":"markdown","source":["#### Rename Column\n\nAlthough we can rename a column in the manner that we just described, another alternative is to\nuse the **withColumnRenamed** method. This will rename the column with the name of the string in\nthe first argument to the string in the second argument"],"metadata":{}},{"cell_type":"code","source":["df.withColumnRenamed(\"DEST_COUNTRY_NAME\",\"Destination_\").columns"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[38]: [&#39;Destination_&#39;, &#39;ORIGIN_COUNTRY_NAME&#39;, &#39;count&#39;]</div>"]}}],"execution_count":48},{"cell_type":"markdown","source":["##### Case Sensitivity\nBy default Spark is case insensitive; however, you can make Spark case sensitive by setting the\nconfiguration:\n\n-- in SQL\n\nset spark.sql.caseSensitive true"],"metadata":{}},{"cell_type":"markdown","source":["#### Removing Columns"],"metadata":{}},{"cell_type":"code","source":["df.columns"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[40]: [&#39;DEST_COUNTRY_NAME&#39;, &#39;ORIGIN_COUNTRY_NAME&#39;, &#39;count&#39;]</div>"]}}],"execution_count":51},{"cell_type":"code","source":["df.drop(\"ORIGIN_COUNTRY_NAME\").columns"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[39]: [&#39;DEST_COUNTRY_NAME&#39;, &#39;count&#39;]</div>"]}}],"execution_count":52},{"cell_type":"code","source":["df.drop(\"ORIGIN_COUNTRY_NAME\", \"count\").columns"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[43]: [&#39;DEST_COUNTRY_NAME&#39;]</div>"]}}],"execution_count":53},{"cell_type":"markdown","source":["#### Changing a Column’s Type (cast)\nSometimes, we might need to convert from one type to another; for example, if we have a set of\nStringType that should be integers. We can convert columns from one type to another by casting the column from one type to another."],"metadata":{}},{"cell_type":"code","source":["df.withColumn(\"integer_casting\", col(\"count\").cast(\"int\")).printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- DEST_COUNTRY_NAME: string (nullable = true)\n-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n-- count: long (nullable = true)\n-- longType_casting: integer (nullable = true)\n\n</div>"]}}],"execution_count":55},{"cell_type":"markdown","source":["#### Filtering Rows\n\nThere are two methods to perform this operation: you can use where or filter\nand they both will perform the same operation and accept the same argument types when used\nwith DataFrames. We will stick to where because of its familiarity to SQL; however, filter is\nvalid as well."],"metadata":{}},{"cell_type":"code","source":["df.filter(col(\"count\") < 2).show(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-------------------+-----+\nDEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n    United States|            Croatia|    1|\n    United States|          Singapore|    1|\n+-----------------+-------------------+-----+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":57},{"cell_type":"code","source":["df.where(\"count < 2\").show(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-------------------+-----+\nDEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n    United States|            Croatia|    1|\n    United States|          Singapore|    1|\n+-----------------+-------------------+-----+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":58},{"cell_type":"markdown","source":["Instinctually, you might want to put **multiple filters** into the same expression. Although this is\npossible, it is not always useful, because Spark automatically performs all filtering operations at\nthe same time regardless of the filter ordering. This means that if you want to specify multiple\nAND filters, just chain them sequentially and let Spark handle the rest:"],"metadata":{}},{"cell_type":"code","source":["# in Python\ndf.where(\"count<2\").where(\"ORIGIN_COUNTRY_NAME != 'Croatia'\").show(2)\n\n# we can also write as follows\ndf.where(col(\"count\")<2).where(col(\"ORIGIN_COUNTRY_NAME\") != 'Croatia').show(2)\n\n# -- in SQL\n# SELECT * FROM dfTable WHERE count < 2 AND ORIGIN_COUNTRY_NAME != \"Croatia\"\n# LIMIT 2"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-------------------+-----+\nDEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n    United States|          Singapore|    1|\n          Moldova|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 2 rows\n\n+-----------------+-------------------+-----+\nDEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n    United States|          Singapore|    1|\n          Moldova|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":60},{"cell_type":"markdown","source":["#### Getting Unique Rows\nA very common use case is to extract the unique or distinct values in a DataFrame. These values\ncan be in one or more columns. The way we do this is by using the distinct method on a\nDataFrame, which allows us to deduplicate any rows that are in that DataFrame. For instance,\nlet’s get the unique origins in our dataset. This, of course, is a transformation that will return a\nnew DataFrame with only unique rows:"],"metadata":{}},{"cell_type":"code","source":["df.select(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\").distinct().count()\n\n\n## -- in SQL\n## SELECT COUNT(DISTINCT(ORIGIN_COUNTRY_NAME, DEST_COUNTRY_NAME)) FROM dfTable"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[61]: 256</div>"]}}],"execution_count":62},{"cell_type":"markdown","source":["#### Random Samples"],"metadata":{}},{"cell_type":"code","source":["# in Python\nseed = 5\nwithReplacement = False\nfraction = 0.5\n\ndf.sample(withReplacement, fraction, seed).count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[62]: 126</div>"]}}],"execution_count":64},{"cell_type":"markdown","source":["#### Random Splits\nRandom splits can be helpful when you need to break up your DataFrame into a random “splits”\nof the original DataFrame. This is often used with machine learning algorithms to create training,\nvalidation, and test sets. In this next example, we’ll split our DataFrame into two different\nDataFrames by setting the weights by which we will split the DataFrame (these are the\narguments to the function). Because this method is designed to be randomized, we will also\nspecify a seed (just replace seed with a number of your choosing in the code block)."],"metadata":{}},{"cell_type":"code","source":["dataFrames = df.randomSplit([0.25,0.75],seed) ## The output is the list"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":66},{"cell_type":"code","source":["type(dataFrames)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[65]: list</div>"]}}],"execution_count":67},{"cell_type":"code","source":["len(dataFrames)  ## it is composed of two list"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[67]: 2</div>"]}}],"execution_count":68},{"cell_type":"code","source":["type(dataFrames[0])   ## its a spark dataframe"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[69]: pyspark.sql.dataframe.DataFrame</div>"]}}],"execution_count":69},{"cell_type":"code","source":["print('test: '+str(dataFrames[0].count()))\n\nprint('train: '+str(dataFrames[1].count()))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">test: 60\ntrain: 196\n</div>"]}}],"execution_count":70},{"cell_type":"markdown","source":["#### Concatenating and Appending Rows (Union)\nAs you learned in the previous section, DataFrames are immutable. This means users cannot\nappend to DataFrames because that would be changing it. To append to a DataFrame, you must\nunion the original DataFrame along with the new DataFrame. This just concatenates the two\nDataFramess. ***To union two DataFrames, you must be sure that they have the same schema and\nnumber of columns; otherwise, the union will fail.***"],"metadata":{}},{"cell_type":"code","source":["# First let us create a new dataframe. \n# To create a new data frame ourselves we need to use sparkContext() to parallelize the data to get an RDD.\n# that RDD can be converted to dataFrame.\n\n## Firstly, create RDD\nfrom pyspark.sql import Row\nschema = df.schema\nnewRows = [\nRow(\"New Country\", \"Other Country\", 5),\nRow(\"New Country 2\", \"Other Country 3\", 1)\n]\n\nrddNew = spark.sparkContext.parallelize(newRows)\n\n## Secondly, create a dataframe\n\ndfNew =  spark.createDataFrame(rddNew,schema)\n\n\n## Thirdly, union\n\ndfUnion = df.union(dfNew).where(\"count = 1\").where(col(\"ORIGIN_COUNTRY_NAME\") != \"United States\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-------------------+-----+\nDEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n    United States|            Croatia|    1|\n    United States|          Singapore|    1|\n    United States|          Gibraltar|    1|\n    United States|             Cyprus|    1|\n    United States|            Estonia|    1|\n    United States|          Lithuania|    1|\n    United States|           Bulgaria|    1|\n    United States|            Georgia|    1|\n    United States|            Bahrain|    1|\n    United States|   Papua New Guinea|    1|\n    United States|         Montenegro|    1|\n    United States|            Namibia|    1|\n    New Country 2|    Other Country 3|    1|\n+-----------------+-------------------+-----+\n\n</div>"]}}],"execution_count":72},{"cell_type":"markdown","source":["##### one more union example"],"metadata":{}},{"cell_type":"code","source":["li = [[1,2,3], [5,6,7]]\n\nrdd_test = spark.sparkContext.parallelize(li)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":74},{"cell_type":"code","source":["rdd_test.take(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[81]: [[1, 2, 3], [5, 6, 7]]</div>"]}}],"execution_count":75},{"cell_type":"code","source":["df_test = spark.createDataFrame(rdd_test)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":76},{"cell_type":"code","source":["df_test.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+---+---+\n _1| _2| _3|\n+---+---+---+\n  1|  2|  3|\n  5|  6|  7|\n+---+---+---+\n\n</div>"]}}],"execution_count":77},{"cell_type":"markdown","source":["#### Sorting Rows\nWhen we sort the values in a DataFrame, we always want to sort with either the largest or\nsmallest values at the top of a DataFrame. **There are two equivalent operations to do this sort\nand orderBy** that work the exact same way. They accept both column expressions and strings as\nwell as multiple columns. **The default is to sort in ascending order**.\n\n***An advanced tip is to use asc_nulls_first, desc_nulls_first, asc_nulls_last, or\ndesc_nulls_last to specify where you would like your null values to appear in an ordered\nDataFrame.***\n\n***For optimization purposes, it’s sometimes advisable to sort within each partition before another\nset of transformations. You can use the sortWithinPartitions method to do this.***"],"metadata":{}},{"cell_type":"code","source":["#from pyspark.sql.functions import desc, asc\n\ndf.orderBy([\"ORIGIN_COUNTRY_NAME\", \"count\"], ascending=[0, 1]).show()\n\n#-- in SQL : SELECT * FROM dfTable ORDER BY ORIGIN_COUNTRY_NAME DESC, count ASC LIMIT 20\n\n# ascending = 0 means  ==> descending, ascending = 1 means ==> ascending\n\n# refer pyspark documentation for other methods of sorting."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+-------------------+-----+\n   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+--------------------+-------------------+-----+\n       United States|            Vietnam|    2|\n       United States|          Venezuela|  246|\n       United States|            Uruguay|   13|\nSaint Vincent and...|      United States|    1|\n       Cote d&#39;Ivoire|      United States|    1|\n            Suriname|      United States|    1|\n            Djibouti|      United States|    1|\n             Moldova|      United States|    1|\n              Cyprus|      United States|    1|\n              Kosovo|      United States|    1|\n                Iraq|      United States|    1|\n           Indonesia|      United States|    1|\n       New Caledonia|      United States|    1|\n              Zambia|      United States|    1|\n               Malta|      United States|    1|\n        Burkina Faso|      United States|    1|\n            Malaysia|      United States|    2|\n             Croatia|      United States|    2|\n             Hungary|      United States|    2|\n             Georgia|      United States|    2|\n+--------------------+-------------------+-----+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":79},{"cell_type":"code","source":["## An advanced tip is to use asc_nulls_first, desc_nulls_first, asc_nulls_last, or\n## desc_nulls_last to specify where you would like your null values to appear in an ordered\n## DataFrame.\n\n\ndf.orderBy(df[\"DEST_COUNTRY_NAME\"].asc_nulls_first()).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+-------------------+-----+\n   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+--------------------+-------------------+-----+\n             Algeria|      United States|    4|\n              Angola|      United States|   15|\n            Anguilla|      United States|   41|\n Antigua and Barbuda|      United States|  126|\n           Argentina|      United States|  180|\n               Aruba|      United States|  346|\n           Australia|      United States|  329|\n             Austria|      United States|   62|\n          Azerbaijan|      United States|   21|\n             Bahrain|      United States|   19|\n            Barbados|      United States|  154|\n             Belgium|      United States|  259|\n              Belize|      United States|  188|\n             Bermuda|      United States|  183|\n             Bolivia|      United States|   30|\nBonaire, Sint Eus...|      United States|   58|\n              Brazil|      United States|  853|\nBritish Virgin Is...|      United States|  107|\n            Bulgaria|      United States|    3|\n        Burkina Faso|      United States|    1|\n+--------------------+-------------------+-----+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":80},{"cell_type":"markdown","source":["#### Limit\nOftentimes, you might want to restrict what you extract from a DataFrame"],"metadata":{}},{"cell_type":"code","source":["df.limit(6)    ## limit is not an action"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[90]: DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]</div>"]}}],"execution_count":82},{"cell_type":"code","source":["df.limit(6).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-------------------+-----+\nDEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n    United States|            Romania|   15|\n    United States|            Croatia|    1|\n    United States|            Ireland|  344|\n            Egypt|      United States|   15|\n    United States|              India|   62|\n    United States|          Singapore|    1|\n+-----------------+-------------------+-----+\n\n</div>"]}}],"execution_count":83},{"cell_type":"code","source":["# in Python\ndf.orderBy(df[\"count\"].desc()).limit(5).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-------------------+------+\nDEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n+-----------------+-------------------+------+\n    United States|      United States|370002|\n    United States|             Canada|  8483|\n           Canada|      United States|  8399|\n    United States|             Mexico|  7187|\n           Mexico|      United States|  7140|\n+-----------------+-------------------+------+\n\n</div>"]}}],"execution_count":84},{"cell_type":"markdown","source":["#### Repartition and Coalesce\nAnother important optimization opportunity is to partition the data according to some frequently\nfiltered columns, which control the physical layout of data across the cluster including the\npartitioning scheme and the number of partitions.\n***Repartition will incur a full shuffle of the data, regardless of whether one is necessary.*** This\nmeans that you should typically only repartition when the future number of partitions is greater\nthan your current number of partitions or when you are looking to partition by a set of columns.\n***Coalesce, on the other hand, will not incur a full shuffle and will try to combine partitions.***"],"metadata":{}},{"cell_type":"code","source":["## getNumPartitions() is a method of rdd. Thats why we used df.rdd.getNumpartitions()\ndf.rdd.getNumPartitions()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[99]: 1</div>"]}}],"execution_count":86},{"cell_type":"code","source":["## Naive Repartitioning\n\ndf.repartition(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[101]: DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]</div>"]}}],"execution_count":87},{"cell_type":"code","source":["## If you know that you’re going to be filtering by a certain column often, it can be worthrepartitioning based on that column:\n\ndf.repartition(col(\"DEST_COUNTRY_NAME\"))\n\n## You can optionally specify the number of partitions you would like, too:\n\ndf.repartition(5, df.DEST_COUNTRY_NAME)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[103]: DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]</div>"]}}],"execution_count":88},{"cell_type":"code","source":["df_repart = df.repartition(5, df.DEST_COUNTRY_NAME)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":89},{"cell_type":"code","source":["df_repart.rdd.getNumPartitions()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[105]: 5</div>"]}}],"execution_count":90},{"cell_type":"code","source":["## This will use the dataframe partitioned in 5 partiotions(with shuffle) based on the destination country name, and then coalesce them (without a full shuffle):\n\ndf_coal = df_repart.coalesce(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":91},{"cell_type":"code","source":["df_coal.rdd.getNumPartitions()  ## after coalesce we get 2 number of partitions"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[108]: 2</div>"]}}],"execution_count":92},{"cell_type":"markdown","source":["#### Collecting Rows to the Driver\nAs discussed in previous chapters, Spark maintains the state of the cluster in the driver. There are\ntimes when you’ll want to collect some of your data to the driver in order to manipulate it on\nyour local machine.\n\nThus far, we did not explicitly define this operation. However, we used several different methods\nfor doing so that are effectively all the same. ***'collect' gets all data from the entire DataFrame,\n'take' selects the first N rows, and 'show' prints out a number of rows nicely.***"],"metadata":{}},{"cell_type":"code","source":["# in Python\ncollectDF = df.limit(10)         ## limit is not an action"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":94},{"cell_type":"code","source":["collectDF.take(5) # take works with an Integer count"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[110]: [Row(DEST_COUNTRY_NAME=&#39;United States&#39;, ORIGIN_COUNTRY_NAME=&#39;Romania&#39;, count=15),\n Row(DEST_COUNTRY_NAME=&#39;United States&#39;, ORIGIN_COUNTRY_NAME=&#39;Croatia&#39;, count=1),\n Row(DEST_COUNTRY_NAME=&#39;United States&#39;, ORIGIN_COUNTRY_NAME=&#39;Ireland&#39;, count=344),\n Row(DEST_COUNTRY_NAME=&#39;Egypt&#39;, ORIGIN_COUNTRY_NAME=&#39;United States&#39;, count=15),\n Row(DEST_COUNTRY_NAME=&#39;United States&#39;, ORIGIN_COUNTRY_NAME=&#39;India&#39;, count=62)]</div>"]}}],"execution_count":95},{"cell_type":"code","source":["collectDF.show() # this prints it out nicely"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-------------------+-----+\nDEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n    United States|            Romania|   15|\n    United States|            Croatia|    1|\n    United States|            Ireland|  344|\n            Egypt|      United States|   15|\n    United States|              India|   62|\n    United States|          Singapore|    1|\n    United States|            Grenada|   62|\n       Costa Rica|      United States|  588|\n          Senegal|      United States|   40|\n          Moldova|      United States|    1|\n+-----------------+-------------------+-----+\n\n</div>"]}}],"execution_count":96},{"cell_type":"code","source":["collectDF.show(5, False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-------------------+-----+\nDEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\nUnited States    |Romania            |15   |\nUnited States    |Croatia            |1    |\nUnited States    |Ireland            |344  |\nEgypt            |United States      |15   |\nUnited States    |India              |62   |\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":97},{"cell_type":"markdown","source":["show(n=20, truncate=True, vertical=False)\n\n###### Refer Documentation:\nhttps://spark.apache.org/docs/latest/api/python/pyspark.sql.html\n\nPrints the first n rows to the console.\n\n\n\nParameters\nn – Number of rows to show.\n\n\n\ntruncate – If set to True, truncate strings longer than 20 chars by default. If set to a number greater than one, truncates long strings to length truncate and align cells right.\n\n\n\nvertical – If set to True, print output rows vertically (one line per column value)."],"metadata":{}},{"cell_type":"code","source":["collectDF.collect()          ## collect is an action"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[113]: [Row(DEST_COUNTRY_NAME=&#39;United States&#39;, ORIGIN_COUNTRY_NAME=&#39;Romania&#39;, count=15),\n Row(DEST_COUNTRY_NAME=&#39;United States&#39;, ORIGIN_COUNTRY_NAME=&#39;Croatia&#39;, count=1),\n Row(DEST_COUNTRY_NAME=&#39;United States&#39;, ORIGIN_COUNTRY_NAME=&#39;Ireland&#39;, count=344),\n Row(DEST_COUNTRY_NAME=&#39;Egypt&#39;, ORIGIN_COUNTRY_NAME=&#39;United States&#39;, count=15),\n Row(DEST_COUNTRY_NAME=&#39;United States&#39;, ORIGIN_COUNTRY_NAME=&#39;India&#39;, count=62),\n Row(DEST_COUNTRY_NAME=&#39;United States&#39;, ORIGIN_COUNTRY_NAME=&#39;Singapore&#39;, count=1),\n Row(DEST_COUNTRY_NAME=&#39;United States&#39;, ORIGIN_COUNTRY_NAME=&#39;Grenada&#39;, count=62),\n Row(DEST_COUNTRY_NAME=&#39;Costa Rica&#39;, ORIGIN_COUNTRY_NAME=&#39;United States&#39;, count=588),\n Row(DEST_COUNTRY_NAME=&#39;Senegal&#39;, ORIGIN_COUNTRY_NAME=&#39;United States&#39;, count=40),\n Row(DEST_COUNTRY_NAME=&#39;Moldova&#39;, ORIGIN_COUNTRY_NAME=&#39;United States&#39;, count=1)]</div>"]}}],"execution_count":99},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":100}],"metadata":{"name":"Chapter 5: Basic Structured Operations","notebookId":2018190601545713},"nbformat":4,"nbformat_minor":0}
