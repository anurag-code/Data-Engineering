{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f1DklZE5h0CE"
   },
   "outputs": [],
   "source": [
    "# Copyright 2019 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wbMLRkw5jn8U"
   },
   "source": [
    "# **Music Recommendation using AutoML Tables**\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.sandbox.google.com/github/GoogleCloudPlatform/python-docs-samples/blob/master/tables/automl/notebooks/music_recommendation/music_recommendation.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/tables/automl/notebooks/music_recommendation/music_recommendation.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m05x0iy4jqDY"
   },
   "source": [
    "## **Overview**\n",
    "\n",
    "In this notebook we will see how [AutoML Tables](https://cloud.google.com/automl-tables/) can be used to make music recommendations to users. AutoML Tables is a supervised learning service for structured data that can vastly simplify the model building process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y7P5m2M-A1yJ"
   },
   "source": [
    "### **Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5SYExJ4XAsRA"
   },
   "source": [
    "AutoML Tables allows data to be imported from either GCS or BigQuery. This tutorial uses the [ListenBrainz](https://console.cloud.google.com/marketplace/details/metabrainz/listenbrainz) dataset from [Cloud Marketplace](https://console.cloud.google.com/marketplace), hosted in BigQuery.\n",
    "\n",
    "The ListenBrainz dataset is a log of songs played by users, some notable pieces of the schema include:\n",
    "\n",
    "##### **Data Schema**\n",
    "\n",
    "<table align=\"left\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th> Field name </th>\n",
    "      <th> Description </th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td><b>user_name</b></td>\n",
    "      <td>a user id</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>track_name</b></td>\n",
    "      <td> a song id</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>release_name</b></td>\n",
    "      <td>the album of the song</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>artist_name</b></td>\n",
    "      <td>the artist of the song</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>tags</b></td>\n",
    "      <td>the genres of the song</td>\n",
    "    </tr>                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "onpOnZOjBBIT"
   },
   "source": [
    "### **Objective**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NdRE10JUAjvX"
   },
   "source": [
    "The goal of this notebook is to demonstrate how to create a lookup table in BigQuery of songs to recommend to users using a log of user-song listens and AutoML Tables. This will be done by training a binary classification model to predict whether or not a user will like a given song. In the training data, liking a song was defined as having listened to a song more than twice. **Using the predictions for every `(user, song)` pair to generate a ranking of the most similar songs for each user.**\n",
    "\n",
    "As the number of `(user, song)` pairs grows exponentially with the number of unique users and songs, this approach may not be optimal for extremely large datasets. One workaround would be to train a model that learns to embed users and songs in the same embedding space, and use a nearest-neighbors algorithm to get recommendations for users. Unfortunately, AutoML Tables does not expose any feature for training and using embeddings, so a [custom ML model](https://github.com/GoogleCloudPlatform/professional-services/tree/master/examples/cloudml-collaborative-filtering) would need to be used instead.\n",
    "\n",
    "Another recommendation approach that is worth mentioning is [using extreme multiclass classification](https://ai.google/research/pubs/pub45530), as that also circumvents storing every possible pair of users and songs. Unfortunately, AutoML Tables does not support the multiclass classification of more than [100 classes](https://cloud.google.com/automl-tables/docs/prepare#target-requirements)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w4YELJp6O_xw"
   },
   "source": [
    "### **Costs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "74OP8KFwO_gs"
   },
   "source": [
    "This tutorial uses billable components of Google Cloud Platform (GCP):\n",
    "\n",
    "* Cloud AI Platform\n",
    "* Bigquery\n",
    "* AutoML Tables\n",
    "\n",
    "Learn about [Cloud AI Platform pricing](https://cloud.google.com/ml-engine/docs/pricing), [Bigquery pricing](https://cloud.google.com/bigquery/pricing), [AutoML Tables pricing](https://cloud.google.com/automl-tables/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9kJu-Wz6OI2W"
   },
   "source": [
    "## **Set up your local development environment**\n",
    "\n",
    "**If you are using Colab or AI Platform Notebooks**, your environment already meets\n",
    "all the requirements to run this notebook. If you are using **AI Platform Notebook**, make sure the machine configuration type is **1 vCPU, 3.75 GB RAM** or above. You can skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FdpoSWy_OMm-"
   },
   "source": [
    "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
    "You need the following:\n",
    "\n",
    "* The Google Cloud SDK\n",
    "* Git\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* Jupyter notebook running in a virtual environment with Python 3\n",
    "\n",
    "The Google Cloud guide to [Setting up a Python development\n",
    "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
    "installation guide](https://jupyter.org/install) provide detailed instructions\n",
    "for meeting these requirements. The following steps provide a condensed set of\n",
    "instructions:\n",
    "\n",
    "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "2. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "3. [Install\n",
    "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
    "   and create a virtual environment that uses Python 3.\n",
    "\n",
    "4. Activate that environment and run `pip install jupyter` in a shell to install\n",
    "   Jupyter.\n",
    "\n",
    "5. Run `jupyter notebook` in a shell to launch Jupyter.\n",
    "\n",
    "6. Open this notebook in the Jupyter Notebook Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dgpdHTag9aUC"
   },
   "source": [
    "## **Set up your GCP project**\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a GCP project.](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "2. [Make sure that billing is enabled for your project.](https://cloud.google.com/billing/docs/how-to/modify-project)\n",
    "\n",
    "3. [Enable the AI Platform APIs and Compute Engine APIs.](https://console.cloud.google.com/flows/enableapi?apiid=ml.googleapis.com,compute_component)\n",
    "\n",
    "4. [Enable AutoML API.](https://console.cloud.google.com/apis/library/automl.googleapis.com?q=automl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Plf6qTgXyYSx"
   },
   "source": [
    "## **PIP Install Packages and dependencies**\n",
    "\n",
    "Install addional dependencies not installed in the notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gt5peqa-h9MO"
   },
   "outputs": [],
   "source": [
    "# Use the latest major GA version of the framework.\n",
    "! pip install --upgrade --quiet --user google-cloud-automl \n",
    "! pip install --upgrade --quiet --user google-cloud-bigquery\n",
    "! pip install --upgrade --quiet --user seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kK5JATKPNf3I"
   },
   "source": [
    "**Note:** Try installing using `sudo`, if the above command throw any permission errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YeQtfJyL-fKp"
   },
   "source": [
    "`Restart` the kernel to allow automl_v1beta1 to be imported for Jupyter Notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ip6IboKF-rQd"
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vLNVYkQF9QLy"
   },
   "source": [
    "## **Set up your GCP Project Id**\n",
    "\n",
    "Enter your `Project Id` in the cell below. Then run the  cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7rG4S9q1Pjfg"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\" #@param {type:\"string\"}\n",
    "COMPUTE_REGION = \"us-central1\" # Currently only supported region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dr--iN2kAylZ"
   },
   "source": [
    "## **Authenticate your GCP account**\n",
    "\n",
    "**If you are using AI Platform Notebooks**, your environment is already\n",
    "authenticated. Skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3yyVCJHFSEKG"
   },
   "source": [
    "Otherwise, follow these steps:\n",
    "\n",
    "1. In the GCP Console, go to the [**Create service account key**\n",
    "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
    "\n",
    "2. From the **Service account** drop-down list, select **New service account**.\n",
    "\n",
    "3. In the **Service account name** field, enter a name.\n",
    "\n",
    "4. From the **Role** drop-down list, select\n",
    "   **AutoML > AutoML Admin** and **BigQuery > BigQuery Admin**.\n",
    "\n",
    "5. Click *Create*. A JSON file that contains your key downloads to your\n",
    "local environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yt6PhVG0UdF1"
   },
   "source": [
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q5TeVHKDMOJF"
   },
   "outputs": [],
   "source": [
    "# Upload the downloaded JSON file that contains your key.\n",
    "import sys\n",
    "\n",
    "if 'google.colab' in sys.modules:    \n",
    "  from google.colab import files\n",
    "  keyfile_upload = files.upload()\n",
    "  keyfile = list(keyfile_upload.keys())[0]\n",
    "  %env GOOGLE_APPLICATION_CREDENTIALS $keyfile\n",
    "  ! gcloud auth activate-service-account --key-file $keyfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d1bnPeDVMR5Q"
   },
   "source": [
    "***If you are running the notebook locally***, enter the path to your service account key as the `GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fsVNKXESYoeQ"
   },
   "outputs": [],
   "source": [
    "# If you are running this notebook locally, replace the string below with the\n",
    "# path to your service account key and run this cell to authenticate your GCP\n",
    "# account.\n",
    "\n",
    "%env GOOGLE_APPLICATION_CREDENTIALS /path/to/service/account\n",
    "! gcloud auth activate-service-account --key-file '/path/to/service/account'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ztLNd4NM1i7C"
   },
   "source": [
    "## **Import libraries and define constants**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LuTnv2o-2oU9"
   },
   "source": [
    "Import relevant packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vRK0FR332vhR"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V9GQ6Flrn0-B"
   },
   "outputs": [],
   "source": [
    "from google.cloud import automl_v1beta1 as automl\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import exceptions\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BbfLaWRs2TR7"
   },
   "source": [
    "Populate the following cell with the necessary constants and run it to initialize constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cVfhqUK_h0CO"
   },
   "outputs": [],
   "source": [
    "#@title Constants { vertical-output: true }\n",
    "\n",
    "# A name for the AutoML tables Dataset to create.\n",
    "DATASET_DISPLAY_NAME = \"music_rec\" #@param {type: 'string'}\n",
    "# The BigQuery dataset to import data from (doesn't need to exist).\n",
    "BQ_DATASET_NAME = \"music_rec_dataset\" #@param {type: 'string'}\n",
    "# The BigQuery table to import data from (doesn't need to exist).\n",
    "BQ_TABLE_NAME = \"music_rec_table\" #@param {type: 'string'}\n",
    "# A name for the AutoML tables model to create.\n",
    "MODEL_DISPLAY_NAME = \"music_rec_model\" #@param {type: 'string'}\n",
    "\n",
    "assert all([\n",
    "    PROJECT_ID,\n",
    "    COMPUTE_REGION,\n",
    "    DATASET_DISPLAY_NAME,\n",
    "    BQ_DATASET_NAME,\n",
    "    BQ_TABLE_NAME,\n",
    "    MODEL_DISPLAY_NAME,\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NI_N8n1PC_5l"
   },
   "source": [
    "Initialize the clients for AutoML, AutoML Tables and BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vwYslwXfDBLy"
   },
   "outputs": [],
   "source": [
    "# Initialize the clients.\n",
    "automl_client = automl.AutoMlClient()\n",
    "tables_client = automl.TablesClient(project=PROJECT_ID, region=COMPUTE_REGION)\n",
    "bq_client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eTKoTeWFxycI"
   },
   "source": [
    "## **Test the set up**\n",
    "\n",
    "To test whether your project set up and authentication steps were successful, run the following cell to list your datasets in this project.\n",
    "\n",
    "If no dataset has previously imported into AutoML Tables, you shall expect an empty return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q9CAYGSNx47m"
   },
   "outputs": [],
   "source": [
    "# List the datasets.\n",
    "list_datasets = tables_client.list_datasets()\n",
    "datasets = { dataset.display_name: dataset.name for dataset in list_datasets }\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0kg5GCSYx0ez"
   },
   "source": [
    "You can also print the list of your models by running the following cell.\n",
    "\n",
    "If no model has previously trained using AutoML Tables, you shall expect an empty return.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jx9Ywkc8x7tK"
   },
   "outputs": [],
   "source": [
    "# List the models.\n",
    "list_models = tables_client.list_models()\n",
    "models = { model.display_name: model.name for model in list_models }\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlE7vCis70xW"
   },
   "source": [
    "## **Import training data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H8oza7faPfEp"
   },
   "source": [
    "### **Create dataset**\n",
    "\n",
    "In order to train a model, a structured dataset must be injested into AutoML tables from either BigQuery or Google Cloud Storage. Once injested, the user will be able to cherry pick columns to use as features, labels, or weights and configure the loss function.\n",
    "\n",
    "#### **Create BigQuery table**\n",
    "First, do some feature engineering on the original ListenBrainz dataset to turn it into a dataset for training and export it into a seperate BigQuery table:\n",
    "\n",
    "    1. Make each sample a unique `(user, song)` pair.\n",
    "    2. For features, use the user's top 10 songs ever played and the song's number of albums, artist, and genres.\n",
    "    3. For a label, use the number of times the user has listened to the song, normalized by dividing by the maximum number of times that user has listened to any song. Normalizing the listen counts ensures active users don't have disproportionate effect on the model error.\n",
    "    4. Add a weight equal to the label to give songs more popular with the user higher weights. This is to help account for the skew in the label distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "snMU9Vd_h0CW"
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "   WITH\n",
    "    songs AS (\n",
    "      SELECT CONCAT(track_name, \" by \", artist_name) AS song,\n",
    "        MAX(tags) as tags\n",
    "      FROM `listenbrainz.listenbrainz.listen`\n",
    "      GROUP BY song\n",
    "      HAVING tags != \"\"\n",
    "      ORDER BY COUNT(*) DESC\n",
    "      LIMIT 10000\n",
    "    ),\n",
    "    user_songs AS (\n",
    "      SELECT user_name AS user, ANY_VALUE(artist_name) AS artist,\n",
    "        CONCAT(track_name, \" by \", artist_name) AS song,\n",
    "        SPLIT(ANY_VALUE(songs.tags), \",\") AS tags,\n",
    "        COUNT(*) AS user_song_listens\n",
    "      FROM `listenbrainz.listenbrainz.listen`\n",
    "      JOIN songs ON songs.song = CONCAT(track_name, \" by \", artist_name)\n",
    "      GROUP BY user_name, song\n",
    "    ),\n",
    "    user_tags AS (\n",
    "      SELECT user, tag, COUNT(*) AS COUNT\n",
    "      FROM user_songs,\n",
    "      UNNEST(tags) tag\n",
    "      WHERE tag != \"\"\n",
    "      GROUP BY user, tag\n",
    "    ),\n",
    "    top_tags AS (\n",
    "      SELECT tag\n",
    "      FROM user_tags\n",
    "      GROUP BY tag\n",
    "      ORDER BY SUM(count) DESC\n",
    "      LIMIT 20\n",
    "    ),\n",
    "    tag_table AS (\n",
    "      SELECT user, b.tag\n",
    "      FROM user_tags a, top_tags b\n",
    "      GROUP BY user, b.tag\n",
    "    ),\n",
    "    user_tag_features AS (\n",
    "      SELECT user,\n",
    "        ARRAY_AGG(IFNULL(count, 0) ORDER BY tag) as user_tags,\n",
    "        SUM(count) as tag_count\n",
    "      FROM tag_table\n",
    "      LEFT JOIN user_tags USING (user, tag)\n",
    "      GROUP BY user\n",
    "    ), user_features AS (\n",
    "      SELECT user, MAX(user_song_listens) AS user_max_listen,\n",
    "        ANY_VALUE(user_tags)[OFFSET(0)]/ANY_VALUE(tag_count) as user_tags0,\n",
    "        ANY_VALUE(user_tags)[OFFSET(1)]/ANY_VALUE(tag_count) as user_tags1,\n",
    "        ANY_VALUE(user_tags)[OFFSET(2)]/ANY_VALUE(tag_count) as user_tags2,\n",
    "        ANY_VALUE(user_tags)[OFFSET(3)]/ANY_VALUE(tag_count) as user_tags3,\n",
    "        ANY_VALUE(user_tags)[OFFSET(4)]/ANY_VALUE(tag_count) as user_tags4,\n",
    "        ANY_VALUE(user_tags)[OFFSET(5)]/ANY_VALUE(tag_count) as user_tags5,\n",
    "        ANY_VALUE(user_tags)[OFFSET(6)]/ANY_VALUE(tag_count) as user_tags6,\n",
    "        ANY_VALUE(user_tags)[OFFSET(7)]/ANY_VALUE(tag_count) as user_tags7,\n",
    "        ANY_VALUE(user_tags)[OFFSET(8)]/ANY_VALUE(tag_count) as user_tags8,\n",
    "        ANY_VALUE(user_tags)[OFFSET(9)]/ANY_VALUE(tag_count) as user_tags9,\n",
    "        ANY_VALUE(user_tags)[OFFSET(10)]/ANY_VALUE(tag_count) as user_tags10,\n",
    "        ANY_VALUE(user_tags)[OFFSET(11)]/ANY_VALUE(tag_count) as user_tags11,\n",
    "        ANY_VALUE(user_tags)[OFFSET(12)]/ANY_VALUE(tag_count) as user_tags12,\n",
    "        ANY_VALUE(user_tags)[OFFSET(13)]/ANY_VALUE(tag_count) as user_tags13,\n",
    "        ANY_VALUE(user_tags)[OFFSET(14)]/ANY_VALUE(tag_count) as user_tags14,\n",
    "        ANY_VALUE(user_tags)[OFFSET(15)]/ANY_VALUE(tag_count) as user_tags15,\n",
    "        ANY_VALUE(user_tags)[OFFSET(16)]/ANY_VALUE(tag_count) as user_tags16,\n",
    "        ANY_VALUE(user_tags)[OFFSET(17)]/ANY_VALUE(tag_count) as user_tags17,\n",
    "        ANY_VALUE(user_tags)[OFFSET(18)]/ANY_VALUE(tag_count) as user_tags18,\n",
    "        ANY_VALUE(user_tags)[OFFSET(19)]/ANY_VALUE(tag_count) as user_tags19\n",
    "      FROM user_songs\n",
    "      LEFT JOIN user_tag_features USING (user)\n",
    "      GROUP BY user\n",
    "      HAVING COUNT(*) < 5000 AND user_max_listen > 2\n",
    "    ),\n",
    "    item_features AS (\n",
    "      SELECT CONCAT(track_name, \" by \", artist_name) AS song,\n",
    "        COUNT(DISTINCT(release_name)) AS albums\n",
    "      FROM `listenbrainz.listenbrainz.listen`\n",
    "      WHERE track_name != \"\"\n",
    "      GROUP BY song\n",
    "    )\n",
    "  SELECT user, song, artist, tags, albums, user_tags0, user_tags1, user_tags2, \n",
    "    user_tags3, user_tags4, user_tags5, user_tags6, user_tags7, user_tags8, \n",
    "    user_tags9, user_tags10, user_tags11, user_tags12, user_tags13, user_tags14, \n",
    "    user_tags15, user_tags16, user_tags17, user_tags18, user_tags19,\n",
    "    IF(user_song_listens > 2, \n",
    "       SQRT(user_song_listens/user_max_listen), \n",
    "       .5/user_song_listens) AS weight,\n",
    "    IF(user_song_listens > 2, 1, 0) as label\n",
    "  FROM user_songs\n",
    "  JOIN user_features USING(user)\n",
    "  JOIN item_features USING(song)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aPIpyqFwh0CY"
   },
   "outputs": [],
   "source": [
    "def create_table_from_query(query, table):\n",
    "    \"\"\"Creates a new table using the results from the given query.\n",
    "    \n",
    "    Args:\n",
    "        query: a query string.\n",
    "        table: a name to give the new table.\n",
    "    \"\"\"\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    bq_dataset = bigquery.Dataset(\"{0}.{1}\".format(\n",
    "        PROJECT_ID, BQ_DATASET_NAME))\n",
    "    bq_dataset.location = \"US\"\n",
    "\n",
    "    try:\n",
    "        bq_dataset = bq_client.create_dataset(bq_dataset)\n",
    "    except exceptions.Conflict:\n",
    "        pass\n",
    "\n",
    "    table_ref = bq_client.dataset(BQ_DATASET_NAME).table(table)\n",
    "    job_config.destination = table_ref\n",
    "\n",
    "    query_job = bq_client.query(query,\n",
    "                             location=bq_dataset.location,\n",
    "                             job_config=job_config)\n",
    "\n",
    "    query_job.result()\n",
    "    print('Query results loaded to table {}'.format(table_ref.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ac0TZQxah0Cb"
   },
   "outputs": [],
   "source": [
    "create_table_from_query(query, BQ_TABLE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tee_qs5xBYQP"
   },
   "source": [
    "### **Create AutoML Dataset**\n",
    "\n",
    "Create a Dataset by importing the BigQuery table that was just created. Importing data may take a few minutes or hours depending on the size of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pah1WO43h0Cd"
   },
   "outputs": [],
   "source": [
    "# Create dataset.\n",
    "dataset = tables_client.create_dataset(\n",
    "    dataset_display_name=DATASET_DISPLAY_NAME)\n",
    "dataset_name = dataset.name\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6ujokeldxof"
   },
   "source": [
    "#### **Import Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NL65mUYtkIgF"
   },
   "outputs": [],
   "source": [
    "# Read the data source from BigQuery. \n",
    "dataset_bq_input_uri = 'bq://{0}.{1}.{2}'.format(\n",
    "    PROJECT_ID, BQ_DATASET_NAME, BQ_TABLE_NAME)\n",
    "\n",
    "import_data_response = tables_client.import_data(\n",
    "    dataset=dataset, bigquery_input_uri=dataset_bq_input_uri)\n",
    "\n",
    "print('Dataset import operation: {}'.format(import_data_response.operation))\n",
    "\n",
    "# Synchronous check of operation status. Wait until import is done.\n",
    "print('Dataset import response: {}'.format(import_data_response.result()))\n",
    "\n",
    "# Verify the status by checking the example_count field.\n",
    "dataset = tables_client.get_dataset(dataset_name=dataset_name)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wej9Lput2k-l"
   },
   "source": [
    "Inspect the datatypes assigned to each column. In this case, the `song` and `artist` should be categorical, not textual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HSUZgHDZh0Cg"
   },
   "outputs": [],
   "source": [
    "list_column_specs_response = tables_client.list_column_specs(\n",
    "    dataset_display_name=DATASET_DISPLAY_NAME)\n",
    "column_specs = {s.display_name: s for s in list_column_specs_response}\n",
    "\n",
    "def print_column_specs(column_specs):\n",
    "    \"\"\"Parses the given specs and prints each column and column type.\"\"\"\n",
    "    data_types = automl.proto.data_types_pb2\n",
    "    return [(x, data_types.TypeCode.Name(\n",
    "        column_specs[x].data_type.type_code)) for x in column_specs.keys()]\n",
    "\n",
    "print_column_specs(column_specs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yD4AwSPGC_PR"
   },
   "source": [
    "## **Update Dataset params**\n",
    "\n",
    "Sometimes, the types AutoML Tables automatically assigns each column will be off from that they were intended to be. When that happens, we need to update Tables with different types for certain columns.\n",
    "\n",
    "In this case, set the `song` and `artist` column types to `CATEGORY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RH7sIHK-h0Ci"
   },
   "outputs": [],
   "source": [
    "type_code='CATEGORY' #@param {type:'string'}\n",
    "\n",
    "for col in [\"song\", \"artist\"]:\n",
    "    tables_client.update_column_spec(dataset_display_name=DATASET_DISPLAY_NAME,\n",
    "                                     column_spec_display_name=col,\n",
    "                                     type_code=type_code)\n",
    "\n",
    "list_column_specs_response = tables_client.list_column_specs(\n",
    "    dataset_display_name=DATASET_DISPLAY_NAME)\n",
    "column_specs = {s.display_name: s for s in list_column_specs_response}\n",
    "print_column_specs(column_specs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fbaQF2iUbbYf"
   },
   "source": [
    "Not all columns are feature columns, in order to train a model, we need to tell Tables which column should be used as the target variable and, optionally, which column should be used as sample weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6p4nCgIXh0Cl"
   },
   "outputs": [],
   "source": [
    "tables_client.set_target_column(dataset_display_name=DATASET_DISPLAY_NAME,\n",
    "                                column_spec_display_name=\"label\")\n",
    "\n",
    "tables_client.set_weight_column(dataset_display_name=DATASET_DISPLAY_NAME,\n",
    "                                column_spec_display_name=\"weight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oM1ssFQrDKEt"
   },
   "source": [
    "## **Create a Model**\n",
    "\n",
    "Once the Dataset has been configured correctly, we can tell AutoML Tables to train a new model. The amount of resources spent to train this model can be adjusted using a parameter called `'train_budget_milli_node_hours'`. As the name implies, this puts a maximum budget on how many resources a training job can use up before exporting a servable model.\n",
    "\n",
    "For demonstration purpose, the following command sets the budget as 1 node hour `('train_budget_milli_node_hours': 1000)`. You can increase that number up to a maximum of 72 hours `('train_budget_milli_node_hours': 72000)` for the best model performance.\n",
    "\n",
    "Even with a budget of 1 node hour (the minimum possible budget), training a model can take more than the specified node hours.\n",
    "\n",
    "You can also select the objective to optimize your model training by setting optimization_objective. This solution optimizes the model by using default optimization objective. Refer [link](https://cloud.google.com/automl-tables/docs/train#opt-obj) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rL6De6ZOh0Co"
   },
   "outputs": [],
   "source": [
    "# The number of hours to train the model.\n",
    "model_train_hours = 1 #@param {type:'integer'}\n",
    "\n",
    "create_model_response = tables_client.create_model(\n",
    "    model_display_name=MODEL_DISPLAY_NAME,\n",
    "    dataset_display_name=DATASET_DISPLAY_NAME,\n",
    "    train_budget_milli_node_hours=model_train_hours*1000)\n",
    "\n",
    "operation_id = create_model_response.operation.name\n",
    "\n",
    "print('Create model operation: {}'.format(create_model_response.operation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4AB-X7q-DKuC"
   },
   "outputs": [],
   "source": [
    "# Wait until model training is done.\n",
    "model = create_model_response.result()\n",
    "model_name = model.name\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L-dkCJ0mDUb9"
   },
   "source": [
    "## **Model Evaluation**\n",
    "\n",
    "Because we are optimizing a surrogate problem (predicting the similarity between `(user, song)` pairs) in order to achieve our final objective of producing a list of recommended songs for a user, it's difficult to tell how well the model performs by looking only at the final loss function. Instead, an evaluation metric we can use for our model is `recall@n` for the top `m` most listened to songs for each user. This metric will give us the probability that one of a user's top `m` most listened to songs will appear in the top `n` recommendations we make.\n",
    "\n",
    "In order to get the top recommendations for each user, we need to create a batch job to predict similarity scores between each user and item pair. These similarity scores would then be sorted per user to produce an ordered list of recommended songs.\n",
    "\n",
    "### **Create an evaluation table**\n",
    "\n",
    "Instead of creating a lookup table for all users, let's just focus on the performance for a few users for this demo. We will focus especially on recommendations for the user `rob`, and demonstrate how the others can be included in an overall evaluation metric for the model. We start by creatings a dataset for prediction to feed into the trained model; this is a table of every possible `(user, song)` pair containing the users and corresponding features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i8Q-71udh0Cs"
   },
   "outputs": [],
   "source": [
    "users = [\"rob\", \"fiveofoh\", \"Aerion\"]\n",
    "training_table = \"{}.{}.{}\".format(\n",
    "                  PROJECT_ID, BQ_DATASET_NAME, BQ_TABLE_NAME)\n",
    "query = \"\"\"\n",
    "    WITH user as (\n",
    "      SELECT user, \n",
    "        user_tags0, user_tags1, user_tags2, user_tags3, user_tags4,\n",
    "        user_tags5, user_tags6, user_tags7, user_tags8, user_tags9,\n",
    "        user_tags10,user_tags11, user_tags12, user_tags13, user_tags14,\n",
    "        user_tags15, user_tags16, user_tags17, user_tags18, user_tags19, label\n",
    "      FROM `{0}`\n",
    "      WHERE user in ({1})\n",
    "    )\n",
    "    SELECT ANY_VALUE(a).*, song, ANY_VALUE(artist) as artist,\n",
    "      ANY_VALUE(tags) as tags, ANY_VALUE(albums) as albums\n",
    "    FROM `{0}`, user a\n",
    "    GROUP BY song\n",
    "\"\"\".format(training_table, \",\".join([\"\\\"{}\\\"\".format(x) for x in users]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "695ngNjxh0Cw"
   },
   "outputs": [],
   "source": [
    "eval_table = \"{}_example\".format(BQ_TABLE_NAME)\n",
    "create_table_from_query(query, eval_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vB_AMuVuDzVP"
   },
   "source": [
    "## **Make predictions**\n",
    "\n",
    "Once the prediction table is created, start a batch prediction job. This may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PmLTJeRwh0Cz"
   },
   "outputs": [],
   "source": [
    "preds_bq_input_uri = \"bq://{}.{}.{}\".format(\n",
    "    PROJECT_ID, BQ_DATASET_NAME, eval_table)\n",
    "preds_bq_output_uri = \"bq://{}\".format(PROJECT_ID)\n",
    "response = tables_client.batch_predict(model_display_name=MODEL_DISPLAY_NAME,\n",
    "                                       bigquery_input_uri=preds_bq_input_uri,\n",
    "                                       bigquery_output_uri=preds_bq_output_uri)\n",
    "\n",
    "print('Prediction response: {}'.format(response.result()))\n",
    "output_uri = response.metadata.batch_predict_details\\\n",
    "             .output_info.bigquery_output_dataset\n",
    "print('Output URI: {}'.format(output_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bupHymOIEHUn"
   },
   "source": [
    "With the similarity predictions for rob, we can order by the predictions to get a ranked list of songs to recommend to `rob`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1xI63tbYh0C2"
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "query = \"\"\"\n",
    "    SELECT user, song, tables.score as score, a.label as pred_label,\n",
    "      b.label as true_label\n",
    "    FROM `{}.predictions` a, UNNEST(predicted_label)\n",
    "    LEFT JOIN `{}` b USING(user, song)\n",
    "    WHERE user = \"{}\" AND CAST(tables.value AS INT64) = 1\n",
    "    ORDER BY score DESC\n",
    "    LIMIT {}\n",
    "\"\"\".format(output_uri[5:].replace(\":\", \".\"), training_table, users[0], n)\n",
    "query_job = bq_client.query(query)\n",
    "\n",
    "print(\"Top {} song recommended for {}:\".format(n, users[0]))\n",
    "for idx, row in enumerate(query_job):\n",
    "    print(\"{}.\".format(idx + 1), row[\"song\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8-NfN_GvERhw"
   },
   "source": [
    "## **Evaluate predictions**\n",
    "\n",
    "**Precision@k and Recall@k**\n",
    "\n",
    "To evaluate the recommendations, we can look at the precision@k and recall@k of our predictions for `rob`. Run the cells below to load the recommendations into a pandas dataframe and plot the precisions and recalls at various top-k recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CZJMbvp8h0C4"
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    WITH \n",
    "      top_k AS (\n",
    "        SELECT user, song, label,\n",
    "          ROW_NUMBER() OVER (PARTITION BY user ORDER BY label + weight DESC) as user_rank\n",
    "        FROM `{0}`\n",
    "      )\n",
    "    SELECT user, song, tables.score as score, b.label,\n",
    "      ROW_NUMBER() OVER (ORDER BY tables.score DESC) as rank, user_rank\n",
    "    FROM `{1}.predictions` a, UNNEST(predicted_label)\n",
    "    LEFT JOIN top_k b USING(user, song)\n",
    "    WHERE CAST(tables.value AS INT64) = 1\n",
    "    ORDER BY score DESC\n",
    "\"\"\".format(training_table, output_uri[5:].replace(\":\", \".\"))\n",
    "\n",
    "df = bq_client.query(query).result().to_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7s0ugRWeh0C8"
   },
   "outputs": [],
   "source": [
    "precision_at_k = {}\n",
    "recall_at_k = {}\n",
    "\n",
    "for user in users:\n",
    "    precision_at_k[user] = []\n",
    "    recall_at_k[user] = []\n",
    "    for k in range(1, 1000):\n",
    "        precision = df[\"label\"][:k].sum() / k\n",
    "        recall = df[\"label\"][:k].sum() / df[\"label\"].sum()\n",
    "        precision_at_k[user].append(precision)\n",
    "        recall_at_k[user].append(recall)\n",
    "\n",
    "# plot the precision-recall curve.\n",
    "ax = sns.lineplot(recall_at_k[users[0]], precision_at_k[users[0]])\n",
    "ax.set_title(\"precision-recall curve for varying k\")\n",
    "ax.set_xlabel(\"recall@k\")\n",
    "ax.set_ylabel(\"precision@k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GakhtEfVEim5"
   },
   "source": [
    "Achieving a high precision@k means a large proportion of top-k recommended items are relevant to the user. Recall@k shows what proportion of all relevant items appeared in the top-k recommendations.\n",
    "\n",
    "**Mean Average Precision (MAP)**\n",
    "\n",
    "Precision@k is a good metric for understanding how many relevant recommendations we might make at each top-k. However, we would prefer relevant items to be recommended first when possible and should encode that into our evaluation metric. **Average Precision (AP)** is a running average of precision@k, rewarding recommendations where the revelant items are seen earlier rather than later. When the averaged across all users for some k, the AP metric is called MAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VUTNcwBzh0DA"
   },
   "outputs": [],
   "source": [
    "def calculate_ap(precision):\n",
    "    ap = [precision[0]]\n",
    "    for p in precision[1:]:\n",
    "        ap.append(ap[-1] + p)\n",
    "    ap = [x / (n + 1) for x, n in zip(ap, range(len(ap)))]\n",
    "    return ap\n",
    "\n",
    "ap_at_k = {user: calculate_ap(pk)\n",
    "           for user, pk in precision_at_k.items()}\n",
    "\n",
    "num_k = 500\n",
    "map_at_k = [sum([ap_at_k[user][k] for user in users]) / len(users)\n",
    "            for k in range(num_k)]\n",
    "print(\"MAP@50: {}\".format(map_at_k[49]))\n",
    "\n",
    "# plot average precision.\n",
    "ax = sns.lineplot(range(num_k), map_at_k)\n",
    "ax.set_title(\"MAP@k for varying k\")\n",
    "ax.set_xlabel(\"k\")\n",
    "ax.set_ylabel(\"MAP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1w6CT9kREu_Z"
   },
   "source": [
    "## **Cleaning up**\n",
    "\n",
    "To clean up all GCP resources used in this project, you can [delete the GCP\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "**Delete BigQuery datasets**\n",
    "\n",
    "In order to delete BigQuery tables, make sure the service account linked to this notebook has a role with the bigquery.tables.delete permission such as Big Query Data Owner. The following command displays the current service account.\n",
    "\n",
    "IAM permissions can be adjusted [here](https://console.cloud.google.com/navigation-error;errorUrl=%2Fiam-admin%2Fiam%3Fproject%3Dprj-automl-notebook&folder%3D&organizationId%3D/permissions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pry8_3bxh0DM"
   },
   "outputs": [],
   "source": [
    "# Delete model resource.\n",
    "tables_client.delete_model(model_name=model_name)\n",
    "\n",
    "# Delete dataset resource.\n",
    "tables_client.delete_dataset(dataset_name=dataset_name)\n",
    "\n",
    "# Delete the prediction dataset.\n",
    "dataset_id = str(output_uri[5:].replace(\":\", \".\"))\n",
    "bq_client.delete_dataset(dataset_id, delete_contents=True, not_found_ok=True)\n",
    "\n",
    "# Delete the training dataset.\n",
    "dataset_id = \"{0}.{1}\".format(PROJECT_ID, BQ_DATASET_NAME)\n",
    "bq_client.delete_dataset(dataset_id, delete_contents=True, not_found_ok=True)\n",
    "\n",
    "# If training model is still running, cancel it.\n",
    "automl_client.transport._operations_client.cancel_operation(operation_id)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "music_recommendation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
